{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2be51b2a-cce4-4e78-8029-125dd23395f3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/05/17 10:08:16 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://f1aa854f04b4:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.4.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>spark://master:7077</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>myApp</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7fbcfd338160>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import year, month, countDistinct, lag, col, sum, when\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import isnan, when, count, col\n",
    "from pyspark.sql.functions import sum, col, round\n",
    "\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.types import *\n",
    "from datetime import date\n",
    "# create a SparkSession\n",
    "spark = SparkSession.builder.appName(\"myApp\").master(\"spark://master:7077\").getOrCreate()\n",
    "spark"
   ]
  },
  {
   "cell_type": "raw",
   "id": "41cef200-deb7-45d5-afd2-40cf05c39f22",
   "metadata": {},
   "source": [
    "# Créer une configuration Spark\n",
    "conf = SparkConf()\n",
    "    .set(\"spark.driver.maxResultSize\", \"4g\")  # Définir la taille maximale des résultats du driver à 4 Go\n",
    "    .set(\"spark.sql.broadcastTimeout\", \"300\")  # Définir le délai de diffusion à 300 secondes\n",
    "\n",
    "# Créer une session Spark en utilisant la configuration\n",
    "spark = SparkSession.builder.config(conf=conf).getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d6322d78-ff15-4162-b34d-3725e2fe5c9a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/05/17 10:12:49 WARN TaskSetManager: Lost task 0.0 in stage 0.0 (TID 0) (172.19.0.4 executor 0): org.apache.spark.SparkFileNotFoundException: File file:/data/données/Données_transformés.csv/part-00000-b6283746-28c6-4079-a29f-0aa153221823-c000.csv does not exist\n",
      "It is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved.\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.readCurrentFileNotFoundError(QueryExecutionErrors.scala:794)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:231)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:290)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:125)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:888)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:888)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "\n",
      "23/05/17 10:12:49 ERROR TaskSetManager: Task 0 in stage 0.0 failed 4 times; aborting job\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o30.csv.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 0.0 failed 4 times, most recent failure: Lost task 0.3 in stage 0.0 (TID 3) (172.19.0.4 executor 0): org.apache.spark.SparkFileNotFoundException: File file:/data/données/Données_transformés.csv/part-00000-b6283746-28c6-4079-a29f-0aa153221823-c000.csv does not exist\nIt is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved.\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.readCurrentFileNotFoundError(QueryExecutionErrors.scala:794)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:231)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:290)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:125)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:888)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:888)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:750)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2785)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2721)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2720)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2720)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1206)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1206)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1206)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2984)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2923)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2912)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:971)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2263)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2284)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2303)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:530)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:483)\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:61)\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:4177)\n\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:3161)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4167)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:526)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4165)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:118)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:195)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:103)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:65)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:4165)\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:3161)\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:3382)\n\tat org.apache.spark.sql.execution.datasources.csv.TextInputCSVDataSource$.infer(CSVDataSource.scala:111)\n\tat org.apache.spark.sql.execution.datasources.csv.CSVDataSource.inferSchema(CSVDataSource.scala:64)\n\tat org.apache.spark.sql.execution.datasources.csv.CSVFileFormat.inferSchema(CSVFileFormat.scala:62)\n\tat org.apache.spark.sql.execution.datasources.DataSource.$anonfun$getOrInferFileFormatSchema$11(DataSource.scala:208)\n\tat scala.Option.orElse(Option.scala:447)\n\tat org.apache.spark.sql.execution.datasources.DataSource.getOrInferFileFormatSchema(DataSource.scala:205)\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:407)\n\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:229)\n\tat org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:211)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:211)\n\tat org.apache.spark.sql.DataFrameReader.csv(DataFrameReader.scala:538)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: org.apache.spark.SparkFileNotFoundException: File file:/data/données/Données_transformés.csv/part-00000-b6283746-28c6-4079-a29f-0aa153221823-c000.csv does not exist\nIt is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved.\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.readCurrentFileNotFoundError(QueryExecutionErrors.scala:794)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:231)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:290)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:125)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:888)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:888)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m Base \u001b[38;5;241m=\u001b[39m \u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcsv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mDonnées_transformés.csv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mheader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/pyspark/sql/readwriter.py:727\u001b[0m, in \u001b[0;36mDataFrameReader.csv\u001b[0;34m(self, path, schema, sep, encoding, quote, escape, comment, header, inferSchema, ignoreLeadingWhiteSpace, ignoreTrailingWhiteSpace, nullValue, nanValue, positiveInf, negativeInf, dateFormat, timestampFormat, maxColumns, maxCharsPerColumn, maxMalformedLogPerPartition, mode, columnNameOfCorruptRecord, multiLine, charToEscapeQuoteEscaping, samplingRatio, enforceSchema, emptyValue, locale, lineSep, pathGlobFilter, recursiveFileLookup, modifiedBefore, modifiedAfter, unescapedQuoteHandling)\u001b[0m\n\u001b[1;32m    725\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(path) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mlist\u001b[39m:\n\u001b[1;32m    726\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_spark\u001b[38;5;241m.\u001b[39m_sc\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 727\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_df(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jreader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcsv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_spark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jvm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPythonUtils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtoSeq\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    728\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(path, RDD):\n\u001b[1;32m    730\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfunc\u001b[39m(iterator):\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/pyspark/errors/exceptions/captured.py:169\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    167\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    168\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 169\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    170\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    171\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o30.csv.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 0.0 failed 4 times, most recent failure: Lost task 0.3 in stage 0.0 (TID 3) (172.19.0.4 executor 0): org.apache.spark.SparkFileNotFoundException: File file:/data/données/Données_transformés.csv/part-00000-b6283746-28c6-4079-a29f-0aa153221823-c000.csv does not exist\nIt is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved.\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.readCurrentFileNotFoundError(QueryExecutionErrors.scala:794)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:231)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:290)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:125)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:888)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:888)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:750)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2785)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2721)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2720)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2720)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1206)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1206)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1206)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2984)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2923)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2912)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:971)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2263)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2284)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2303)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:530)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:483)\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:61)\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:4177)\n\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:3161)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4167)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:526)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4165)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:118)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:195)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:103)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:65)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:4165)\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:3161)\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:3382)\n\tat org.apache.spark.sql.execution.datasources.csv.TextInputCSVDataSource$.infer(CSVDataSource.scala:111)\n\tat org.apache.spark.sql.execution.datasources.csv.CSVDataSource.inferSchema(CSVDataSource.scala:64)\n\tat org.apache.spark.sql.execution.datasources.csv.CSVFileFormat.inferSchema(CSVFileFormat.scala:62)\n\tat org.apache.spark.sql.execution.datasources.DataSource.$anonfun$getOrInferFileFormatSchema$11(DataSource.scala:208)\n\tat scala.Option.orElse(Option.scala:447)\n\tat org.apache.spark.sql.execution.datasources.DataSource.getOrInferFileFormatSchema(DataSource.scala:205)\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:407)\n\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:229)\n\tat org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:211)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:211)\n\tat org.apache.spark.sql.DataFrameReader.csv(DataFrameReader.scala:538)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: org.apache.spark.SparkFileNotFoundException: File file:/data/données/Données_transformés.csv/part-00000-b6283746-28c6-4079-a29f-0aa153221823-c000.csv does not exist\nIt is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved.\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.readCurrentFileNotFoundError(QueryExecutionErrors.scala:794)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:231)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:290)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:125)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:888)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:888)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n"
     ]
    }
   ],
   "source": [
    "Base = spark.read.csv('Données_transformés.csv',header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d4ecbe0-d379-4703-8947-7d4a4e6fe1b2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get number of records\n",
    "print(\"The data contain %d records.\" % Base.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65b3fc7f-9f24-462f-8914-9055b3574e94",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get number of columns\n",
    "print(\"The data contain %d columns.\" % len(Base.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b852e7d-9562-4cf8-9278-73e41890ced1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Select randomly 1M records\n",
    "BaseE = Base.sample(False, 0.5, 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b18a369c-0f42-45b4-a647-f9aa37b92628",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"The data contain %d records.\" % BaseE.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9a96c03-9f73-4f94-9831-f41f5ef465be",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "BaseE.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecb724fd-5edb-42cc-b134-778a9e18542c",
   "metadata": {},
   "source": [
    "Data Pre-processing\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deaeff6a-0c3c-46ef-bdda-1d6b19593644",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "### Get count of nan or missing values\n",
    "from pyspark.sql.functions import isnan, when, count, col\n",
    "\n",
    "BaseE.select([count(when(isnan(c), c)).alias(c) for c in BaseE.columns]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe89aa50-bd29-4c26-a592-f0486cd49348",
   "metadata": {},
   "source": [
    "Observations:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc60c03b-52d7-412a-943d-c38c761b35b3",
   "metadata": {},
   "source": [
    "This dataset is clean, no missing values.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9d51181-1e68-4df6-862f-121c09f0bc52",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "Base = Base.withColumnRenamed(\"Accor/Payoff\", \"Accor_Payoff\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b751144c-27cc-4e32-a744-db43500a8436",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "BaseE = BaseE.withColumnRenamed(\"Accor/Payoff\", \"Accor_Payoff\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24bf4425-d849-4f0c-858e-720f6371c10a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Schema of the data\n",
    "BaseE.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51b174ee-50e5-4302-a685-5cdbb9ca8e43",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68be8470-354a-4139-b617-f723a01490bc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "BaseE.select(\"code_client\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b077135-df59-4b23-97ec-c103ebbf01a8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml import Pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86da3759-5f6d-4123-9fa1-7a56e67ec306",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "BaseE.select(\"produit\").show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3f2260a-3e11-48d1-a734-7eb5338e010a",
   "metadata": {
    "tags": []
   },
   "source": [
    "from pyspark.ml.feature import OneHotEncoder\n",
    "\n",
    "# Créer une instance de StringIndexer pour chaque colonne à encoder\n",
    "indexer = StringIndexer(inputCol=\"produit\", outputCol=\"produit_indexed\")\n",
    "\n",
    "# Appliquer la transformation d'indexation\n",
    "indexed = indexer.fit(BaseE).transform(BaseE)\n",
    "\n",
    "# Créer une instance de OneHotEncoder pour chaque colonne indexée\n",
    "encoder = OneHotEncoder(inputCols=[\"produit_indexed\"], outputCols=[\"produitF_encoded\"])\n",
    "\n",
    "# Appliquer la transformation d'encodage one-hot\n",
    "df_encoded = encoder.fit(indexed).transform(indexed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca6cbdb6-40e3-4e0e-a670-878622b056e9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# pip install --quiet scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "907311a8-00c5-453c-9648-b3fa5b578968",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# from sklearn.preprocessing import LabelEncoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29bded35-b867-47b3-abcf-d5085dff321c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a3faa47-fa51-42c7-bb4a-96d7dfc02281",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "columns_to_encode = [\"status_pret\", \"LR_CARTE_BQ\",\"MARITAL_STATUS\",\"GENDER\",\"NATIONALITY\",\"Accor_Payoff\",\"gouvernorat\",\"secteur_activite\",\"valider_par\",\"duree\",\"agence\",\"produit\",\"date_de_deboursement\",\"code_pret\"]\n",
    "\n",
    "# Create a StringIndexer for each column\n",
    "string_indexers = [StringIndexer(inputCol=col, outputCol=col+'_F') for col in columns_to_encode]\n",
    "\n",
    "# Fit and transform the DataFrame for each StringIndexer\n",
    "indexed_data = BaseE\n",
    "for indexer in string_indexers:\n",
    "    indexed_data = indexer.fit(indexed_data).transform(indexed_data)\n",
    "\n",
    "\n",
    "# Show the transformed DataFrame\n",
    "indexed_data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cde897c0-c457-4b95-a550-6e25a8600b57",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Supprimer les colonnes d'origine de indexed_data\n",
    "indexed_data = indexed_data.drop(*columns_to_encode)\n",
    "\n",
    "# Afficher le DataFrame résultant\n",
    "indexed_data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1365344-df87-48e4-aaa9-9fb537d112af",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "indexed_data.select(\"produit_F\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f19ab9b5-65f6-478d-a522-03693167f62d",
   "metadata": {},
   "outputs": [],
   "source": [
    "indexed_data.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4863dd1-e5d4-4a50-8eb2-baed092f257a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import DoubleType\n",
    "\n",
    "indexed_data = indexed_data.withColumn(\"code_client\", indexed_data.code_client.cast(DoubleType()))\n",
    "indexed_data = indexed_data.withColumn(\"TEL_MOBILE\", indexed_data.TEL_MOBILE.cast(DoubleType()))\n",
    "indexed_data = indexed_data.withColumn(\"code_agent\", indexed_data.code_agent.cast(DoubleType()))\n",
    "indexed_data = indexed_data.withColumn(\"Fraude\", indexed_data.Fraude.cast('int'))\n",
    "indexed_data= indexed_data.withColumn(\"montant_demander\", indexed_data.montant_demander.cast(DoubleType()))\n",
    "indexed_data = indexed_data.withColumn(\"montant_accorder\", indexed_data.montant_accorder.cast(DoubleType()))\n",
    "indexed_data = indexed_data.withColumn(\"code_secteur_du_client\", indexed_data.code_secteur_du_client.cast(DoubleType()))\n",
    "indexed_data = indexed_data.withColumn(\"code_secteur_projet\", indexed_data.code_secteur_projet.cast(DoubleType()))\n",
    "indexed_data = indexed_data.withColumn(\"id\", indexed_data.id.cast(DoubleType()))\n",
    "indexed_data = indexed_data.withColumn(\"ACCOUNT_OFFICER\", indexed_data.ACCOUNT_OFFICER.cast(DoubleType()))\n",
    "indexed_data = indexed_data.withColumn(\"INDUSTRY\",indexed_data.INDUSTRY.cast(DoubleType()))\n",
    "indexed_data = indexed_data.withColumn(\"SECTOR\", indexed_data.SECTOR.cast(DoubleType()))\n",
    "indexed_data = indexed_data.withColumn(\"LR_CUS_MNT_REV\", indexed_data.LR_CUS_MNT_REV.cast(DoubleType()))\n",
    "indexed_data = indexed_data.withColumn(\"SUBURB_TOWN\", indexed_data.SUBURB_TOWN.cast(DoubleType()))\n",
    "indexed_data = indexed_data.withColumn(\"AGE\", indexed_data.AGE.cast('int'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "668fb081-bf4a-46d5-ac10-dbeed3e8d5c1",
   "metadata": {},
   "source": [
    "columns_to_encode = [\"status_pret\", \"LR_CARTE_BQ\",\"MARITAL_STATUS\",\"GENDER\",\"NATIONALITY\",\"Accor_Payoff\",\"gouvernorat\",\"secteur_activite\",\"valider_par\",\"duree\",\"agence\",\"produit\",\"date_de_deboursement\",\"code_pret\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec55a6ba-1423-4511-a6b7-4845621e5c2a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Check column data types\n",
    "indexed_data.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49140dfa-fe1f-4f70-86d5-c68036806d91",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "indexed_data.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cb15e03-1661-4f6c-b85d-9a7f9b9a3dab",
   "metadata": {},
   "source": [
    "EDA : Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11b0a5e3-9660-4a23-b0f8-0acf5bfe5f90",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "indexed_data.groupBy(\"Fraude\").count().show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c14090f-5688-466c-9e96-6860d70ef738",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "from pyspark.sql.functions import expr\n",
    "\n",
    "# Créer une session Spark\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "# Supposons que vous ayez un DataFrame appelé \"data\" contenant les données\n",
    "# avec une colonne \"Fraude\" représentant les fraudes (0 pour non fraude, 1 pour fraude)\n",
    "# et d'autres colonnes si nécessaire.\n",
    "\n",
    "# Calculer le nombre total d'enregistrements\n",
    "total_count = indexed_data.count()\n",
    "\n",
    "# Calculer le nombre de fraudes\n",
    "fraude_count = indexed_data.filter(col(\"Fraude\") == 1).count()\n",
    "\n",
    "# Calculer le nombre de non-fraudes\n",
    "non_fraude_count = total_count - fraude_count\n",
    "\n",
    "# Calculer le pourcentage de fraude et de non-fraude\n",
    "pourcentage_fraude = (fraude_count / total_count) * 100\n",
    "pourcentage_non_fraude = (non_fraude_count / total_count) * 100\n",
    "\n",
    "# Afficher les résultats\n",
    "print(\"Pourcentage de fraude: \", pourcentage_fraude)\n",
    "print(\"Pourcentage de non-fraude: \", pourcentage_non_fraude)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b045f3e6-0d90-45d2-a865-de85a764fa83",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Créer les données pour le graphique\n",
    "labels = ['Fraude', 'Non-Fraude']\n",
    "pourcentages = [pourcentage_fraude, pourcentage_non_fraude]\n",
    "\n",
    "# Créer le graphique à secteurs (pie chart)\n",
    "plt.pie(pourcentages, labels=labels, autopct='%1.1f%%')\n",
    "\n",
    "# Ajouter un titre\n",
    "plt.title('Pourcentage de Fraude et de Non-Fraude')\n",
    "\n",
    "# Afficher le graphique\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfb7c7f0-8780-4196-80f9-be9a67d0a69e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81c9e888-7889-409e-b010-12bc7cca927d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "indexed_data.dtypes"
   ]
  },
  {
   "cell_type": "raw",
   "id": "615f40d6-09f4-463d-99c4-a11847fb1f1f",
   "metadata": {
    "tags": []
   },
   "source": [
    "import pandas as pd\n",
    "from pandas.plotting import scatter_matrix\n",
    "\n",
    "numeric_features = ['code_client', 'code_pret_F',\n",
    "                  'produit_F', \n",
    "                    'agence_F',\n",
    "                    'duree_F' ,'status_pret_F', 'GENDER_F', \n",
    "                    'MARITAL_STATUS_F','LR_CARTE_BQ_F', \n",
    "                    'AGE']\n",
    "\n",
    "numeric_data = indexed_data.select(numeric_features).toPandas()\n",
    "axs = pd.plotting.scatter_matrix(numeric_data, figsize=(10,10));\n",
    "n = len(numeric_data.columns)\n",
    "for i in range(n):\n",
    "    v = axs[i, 0]\n",
    "    v.yaxis.label.set_rotation(0)\n",
    "    v.yaxis.label.set_ha('right')\n",
    "    v.set_yticks(())\n",
    "    h = axs[n-1, i]\n",
    "    h.xaxis.label.set_rotation(90)\n",
    "    h.set_xticks(())"
   ]
  },
  {
   "cell_type": "raw",
   "id": "bd16da96-32c4-46b5-a05f-f7cf34b495c0",
   "metadata": {},
   "source": [
    "numeric_features = ['code_client', 'TEL_MOBILE', 'code_pret_F',\n",
    "                    'date_de_deboursement_F','produit_F', \n",
    "                    'agence_F','montant_demander', 'montant_accorder',\n",
    "                    'duree_F','valider_par_F', \n",
    "                    'secteur_activite_F','code_secteur_du_client', 'code_secteur_projet', 'gouvernorat_F',\n",
    "                    'status_pret_F','Accor_Payoff_F', \n",
    "                    'id','ACCOUNT_OFFICER', 'INDUSTRY', 'NATIONALITY_F',\n",
    "                    'GENDER_F','SECTOR', \n",
    "                    'MARITAL_STATUS_F','LR_CARTE_BQ_F', 'LR_CUS_MNT_REV', 'SUBURB_TOWN',\n",
    "                    'AGE']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28cbf822-4360-4cce-b4e2-faefa75ef2ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "assembler = VectorAssembler(inputCols=['code_client', 'TEL_MOBILE', 'code_pret_F',\n",
    "                    'date_de_deboursement_F','produit_F', \n",
    "                    'agence_F','montant_demander', 'montant_accorder',\n",
    "                    'duree_F','valider_par_F', \n",
    "                    'secteur_activite_F','code_secteur_du_client', 'code_secteur_projet', 'gouvernorat_F',\n",
    "                    'status_pret_F','Accor_Payoff_F', \n",
    "                    'id','ACCOUNT_OFFICER', 'INDUSTRY', 'NATIONALITY_F',\n",
    "                    'GENDER_F','SECTOR', \n",
    "                    'MARITAL_STATUS_F','LR_CARTE_BQ_F', 'LR_CUS_MNT_REV', 'SUBURB_TOWN',\n",
    "                    'AGE'], outputCol='features')\n",
    "data = assembler.transform(indexed_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a19953a-584d-4b44-a4ec-87abd1d3efb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['features','Fraude'].show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21f20c02-80eb-4aa2-b4c1-0389c2b9c504",
   "metadata": {},
   "outputs": [],
   "source": [
    "(train, test )= data.randomSplit([0.8, 0.2], seed=10)\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b7c04552-69cf-49cf-8f41-af3f8a5620b8",
   "metadata": {
    "tags": []
   },
   "source": [
    "[train.count(), test.count()]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8e32e08-3627-46a5-8b48-9ce1423a6858",
   "metadata": {},
   "source": [
    "Logistic Regression :\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b108b64-f600-48d3-945c-6edb25fc0274",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "#Create a Logistic Regression classifier.\n",
    "logistic = LogisticRegression(labelCol = \"Fraude\", featuresCol = \"features\")\n",
    "# Learn from the training data.\n",
    "lrModel = logistic.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58944931-6243-4071-bbf7-da764605006a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prediction_LR = lrModel.transform(test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82ec2aff-b819-472e-aa1e-fc22ca7da8ea",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
