{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2be51b2a-cce4-4e78-8029-125dd23395f3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://b0e1bb9c76b4:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.4.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>myApp</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f55bbea9c70>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import year, month, countDistinct, lag, col, sum, when\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import isnan, when, count, col\n",
    "from pyspark.sql.functions import sum, col, round\n",
    "\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.types import *\n",
    "from datetime import date\n",
    "# create a SparkSession\n",
    "spark = SparkSession.builder.appName(\"myApp\").getOrCreate()\n",
    "spark"
   ]
  },
  {
   "cell_type": "raw",
   "id": "41cef200-deb7-45d5-afd2-40cf05c39f22",
   "metadata": {},
   "source": [
    "# Créer une configuration Spark\n",
    "conf = SparkConf()\n",
    "    .set(\"spark.driver.maxResultSize\", \"4g\")  # Définir la taille maximale des résultats du driver à 4 Go\n",
    "    .set(\"spark.sql.broadcastTimeout\", \"300\")  # Définir le délai de diffusion à 300 secondes\n",
    "\n",
    "# Créer une session Spark en utilisant la configuration\n",
    "spark = SparkSession.builder.config(conf=conf).getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d6322d78-ff15-4162-b34d-3725e2fe5c9a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "Base = spark.read.csv('Donntransformes.csv',header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2d4ecbe0-d379-4703-8947-7d4a4e6fe1b2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 2:>                                                          (0 + 4) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The data contain 1372581 records.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Get number of records\n",
    "print(\"The data contain %d records.\" % Base.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "65b3fc7f-9f24-462f-8914-9055b3574e94",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The data contain 29 columns.\n"
     ]
    }
   ],
   "source": [
    "# Get number of columns\n",
    "print(\"The data contain %d columns.\" % len(Base.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3b852e7d-9562-4cf8-9278-73e41890ced1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Select randomly 1M records\n",
    "BaseE = Base.sample(False, 0.5, 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b18a369c-0f42-45b4-a647-f9aa37b92628",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 5:==============>                                            (1 + 3) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The data contain 686558 records.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "print(\"The data contain %d records.\" % BaseE.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d9a96c03-9f73-4f94-9831-f41f5ef465be",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/05/19 18:05:08 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+------------+--------------------+----------+----------+----------------+----------------+----------------+-----+--------------+-------------------+----------------------+-------------------+-----------+-----------+------------+-------+---------------+--------+-----------+------+------+--------------+-----------+--------------+-----------+---+------+\n",
      "|code_client|TEL_MOBILE|   code_pret|date_de_deboursement|   produit|code_agent|          agence|montant_demander|montant_accorder|duree|   valider_par|   secteur_activite|code_secteur_du_client|code_secteur_projet|gouvernorat|status_pret|Accor/Payoff|     id|ACCOUNT_OFFICER|INDUSTRY|NATIONALITY|GENDER|SECTOR|MARITAL_STATUS|LR_CARTE_BQ|LR_CUS_MNT_REV|SUBURB_TOWN|AGE|Fraude|\n",
      "+-----------+----------+------------+--------------------+----------+----------+----------------+----------------+----------------+-----+--------------+-------------------+----------------------+-------------------+-----------+-----------+------------+-------+---------------+--------+-----------+------+------+--------------+-----------+--------------+-----------+---+------+\n",
      "|    2097563|       3.0|AA2114701BWS| 2021-05-27 00:00:00|   Mawilni|      3299|         Soliman|          2500.0|        2000.000|  17M|BARGUELLIL.WA1|           Commerce|                156055|             156055|     Nabeul|      CLOSE|      Payoff|2097563|            469|    3016|         TN|FEMALE|  3000|       MARRIED|         NO|         500.0|     1560.0| 43|     0|\n",
      "|    2097615|       3.0|AA20324QYV32| 2020-11-19 00:00:00|   Mawilni|      2863|      Tajerouine|          1000.0|        2500.000|  25M|   CHERNI.AYM1|         Production|                235552|             235562|     Le Kef|      CLOSE|      Payoff|2097615|            473|    6032|         TN|FEMALE|  6000|       MARRIED|        YES|        9999.0|     2355.0| 28|     1|\n",
      "|    2551368|       3.0|AA19170RY5K0| 2019-06-19 00:00:00|   Mawilni|      2918|        Zaghouan|          2000.0|        1000.000|  10M|  ARFAOUI.KHA1|Agriculture&Elevage|                165151|             165151|   Zaghouan|      CLOSE|      Payoff|2551368|            478|    1014|         TN|  MALE|  1000|       MARRIED|         NO|        1000.0|     1651.0| 40|     0|\n",
      "|    2551451|       3.0|AA19247VC7Q7| 2019-09-05 00:00:00|Mouasasaty|      2227|         El-Krib|         25000.0|       10000.000|  28M|BOUGHANMI.KAR1|           Services|                215551|             215551|       Beja|      CLOSE|    Accorder|2551451|            418|    7005|         TN|FEMALE|  7000|       MARRIED|        YES|         600.0|     2155.0| 58|     0|\n",
      "|    2551527|       3.0|AA191707V5F8| 2019-06-20 00:00:00|   Mawilni|      2805|         Siliana|          5000.0|        5000.000|  25M|ABIDALLAH.SAB1|           Commerce|                245152|             245152|    Siliana|      CLOSE|      Payoff|2551527|            468|    3010|         TN|  MALE|  3000|       MARRIED|         NO|        1500.0|     2451.0| 42|     1|\n",
      "|    2551726|       3.0|AA191701J1N4| 2019-06-20 00:00:00|   Mawilni|      2700|      Oued Ellil|          5000.0|        2000.000|  13M|   NOUIRI.NAJ1|         Production|                145351|             145351|    Manouba|      CLOSE|      Payoff|2551726|            458|    6026|         TN|  MALE|  6000|        SINGLE|        YES|         950.0|     1453.0| 30|     0|\n",
      "|    2551726|       3.0|AA201838M00V| 2020-07-03 00:00:00|   Mawilni|      2700|      Oued Ellil|          5000.0|        4000.000|  17M|   NOUIRI.NAJ1|         Production|                145351|             145351|    Manouba|      CLOSE|    Accorder|2551726|            458|    6026|         TN|  MALE|  6000|        SINGLE|        YES|         950.0|     1453.0| 30|     0|\n",
      "|    2097834|       3.0|AA19330Q0G9T| 2019-11-28 00:00:00|     Darna|      3523|          Tozeur|           700.0|         700.000|  16M|  DAAFOUS.HAD1|              Autre|                625256|             625259|     Tozeur|      CLOSE|    Accorder|2097834|            477|    4005|         TN|FEMALE|  4000|       MARRIED|         NO|        1200.0|     6252.0| 34|     1|\n",
      "|    2551802|       3.0|AA191709DWHV| 2019-06-20 00:00:00|   Mawilni|      2114|     Bizerte Sud|          1000.0|        1000.000|  15M|  ADDASSI.SAB1|           Commerce|                175353|             175353|    Bizerte|      CLOSE|      Payoff|2551802|            507|    3013|         TN|FEMALE|  3000|       MARRIED|        YES|         650.0|     1753.0| 33|     1|\n",
      "|    2097847|       3.0|AA2005061Y6N| 2020-02-19 00:00:00|     Darna|      2704|      Oued Ellil|          3000.0|        3000.000| 652D|    RIAHI.SAW1|              Autre|                145357|             145357|    Manouba|      CLOSE|      Payoff|2097847|            458|    4005|         TN|FEMALE|  4000|       MARRIED|         NO|        1200.0|     1453.0| 62|     1|\n",
      "|    2098019|       3.0|AA220893TK2V| 2022-03-31 00:00:00|    Taalim|      3275|         Enfidha|          1500.0|        1000.000| 344D|  NOURANI.SOF1|              Autre|                316058|             316051|     Sousse|      CLOSE|    Accorder|2098019|            485|    4005|         TN|  MALE|  4000|       MARRIED|        YES|        1000.0|     3160.0| 55|     0|\n",
      "|    2551915|       3.0|AA20261NJ3B8| 2020-09-18 00:00:00|   Mawilni|      2534|Menzel Bourguiba|          2500.0|        2500.000|  13M| CHAALALI.WAJ1|           Commerce|                175856|             175856|    Bizerte|      CLOSE|    Accorder|2551915|            444|    3013|         TN|FEMALE|  3000|       MARRIED|         NO|         400.0|     1758.0| 41|     1|\n",
      "|    2098089|       3.0|AA221057YX16| 2022-04-15 00:00:00|   Mawilni|      3023|         Soliman|          5000.0|        5000.000|  19M|  YAKOUBI.WAL1|           Services|                156154|             156154|     Nabeul|    CURRENT|    Accorder|2098089|            469|    7029|         TN|  MALE|  7000|       MARRIED|        YES|         850.0|     1561.0| 48|     1|\n",
      "|    2551922|       3.0|AA22312KGXW0| 2022-11-08 00:00:00|   Mawilni|      2819|         Soliman|          1500.0|        1100.000|  13M|  YAKOUBI.WAL1|           Services|                156154|             156154|     Nabeul|    CURRENT|    Accorder|2551922|            469|    7027|         TN|  MALE|  7000|       MARRIED|         NO|        1430.0|     1561.0| 65|     0|\n",
      "|    2551922|       3.0|AA19171P7V41| 2019-06-20 00:00:00|     Darna|      2819|         Soliman|          1500.0|        1500.000|  16M|  YAKOUBI.WAL1|           Services|                156154|             156154|     Nabeul|      CLOSE|    Accorder|2551922|            469|    7027|         TN|  MALE|  7000|       MARRIED|         NO|        1430.0|     1561.0| 65|     1|\n",
      "|    2098089|       3.0|AA20343381GT| 2020-12-10 00:00:00|   Mawilni|      3023|         Soliman|          5000.0|        5000.000|  19M|  YAKOUBI.WAL1|           Services|                156154|             156154|     Nabeul|      CLOSE|      Payoff|2098089|            469|    7029|         TN|  MALE|  7000|       MARRIED|        YES|         850.0|     1561.0| 48|     1|\n",
      "|    2551963|       3.0|AA19171KV4TP| 2019-06-20 00:00:00|   Mawilni|      3268|           Gafsa|          3000.0|        3000.000|  13M|    ALOUI.AMM1|           Services|                615458|             615458|      Gafsa|      CLOSE|      Payoff|2551963|            422|    7009|         TN|FEMALE|  7000|       MARRIED|        YES|         864.0|     6154.0| 36|     1|\n",
      "|    2551992|       3.0|AA19171JVCPL| 2019-06-20 00:00:00|   Mawilni|      2087|    Béni Khalled|          3500.0|        1500.000|  10M|KHAMMASSI.SAL1|         Production|                156351|             156351|     Nabeul|      CLOSE|    Accorder|2551992|            408|    6002|         TN|  MALE|  6000|       MARRIED|         NO|        1800.0|     1563.0| 40|     0|\n",
      "|    2098328|       3.0|AA21011T4LRT| 2021-01-11 00:00:00|   Mawssem|      3620|         Sejnene|          1500.0|        3500.000|  15M|MECHERGUI.MEH1|Agriculture&Elevage|                175454|             175454|    Bizerte|      CLOSE|      Payoff|2098328|            484|    1015|         TN|FEMALE|  1000|       MARRIED|        YES|        9999.0|     1754.0| 61|     1|\n",
      "|    2551992|       3.0|AA2110246TV0| 2021-04-12 00:00:00|   Mawilni|      2087|    Béni Khalled|          3500.0|        4000.000| 476D|   AMAYLI.CHA1|         Production|                156351|             156351|     Nabeul|      CLOSE|      Payoff|2551992|            408|    6002|         TN|  MALE|  6000|       MARRIED|         NO|        1800.0|     1563.0| 40|     1|\n",
      "+-----------+----------+------------+--------------------+----------+----------+----------------+----------------+----------------+-----+--------------+-------------------+----------------------+-------------------+-----------+-----------+------------+-------+---------------+--------+-----------+------+------+--------------+-----------+--------------+-----------+---+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "BaseE.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecb724fd-5edb-42cc-b134-778a9e18542c",
   "metadata": {},
   "source": [
    "Data Pre-processing\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "deaeff6a-0c3c-46ef-bdda-1d6b19593644",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 9:============================================>              (3 + 1) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+---------+--------------------+-------+----------+------+----------------+----------------+-----+-----------+----------------+----------------------+-------------------+-----------+-----------+------------+---+---------------+--------+-----------+------+------+--------------+-----------+--------------+-----------+---+------+\n",
      "|code_client|TEL_MOBILE|code_pret|date_de_deboursement|produit|code_agent|agence|montant_demander|montant_accorder|duree|valider_par|secteur_activite|code_secteur_du_client|code_secteur_projet|gouvernorat|status_pret|Accor/Payoff| id|ACCOUNT_OFFICER|INDUSTRY|NATIONALITY|GENDER|SECTOR|MARITAL_STATUS|LR_CARTE_BQ|LR_CUS_MNT_REV|SUBURB_TOWN|AGE|Fraude|\n",
      "+-----------+----------+---------+--------------------+-------+----------+------+----------------+----------------+-----+-----------+----------------+----------------------+-------------------+-----------+-----------+------------+---+---------------+--------+-----------+------+------+--------------+-----------+--------------+-----------+---+------+\n",
      "|          0|         0|        0|                   0|      0|         0|     0|               0|               0|    0|          0|               0|                     0|                  0|          0|          0|           0|  0|              0|       0|          0|     0|     0|             0|          0|             0|          0|  0|     0|\n",
      "+-----------+----------+---------+--------------------+-------+----------+------+----------------+----------------+-----+-----------+----------------+----------------------+-------------------+-----------+-----------+------------+---+---------------+--------+-----------+------+------+--------------+-----------+--------------+-----------+---+------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "### Get count of nan or missing values\n",
    "from pyspark.sql.functions import isnan, when, count, col\n",
    "\n",
    "BaseE.select([count(when(isnan(c), c)).alias(c) for c in BaseE.columns]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe89aa50-bd29-4c26-a592-f0486cd49348",
   "metadata": {},
   "source": [
    "Observations:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc60c03b-52d7-412a-943d-c38c761b35b3",
   "metadata": {},
   "source": [
    "This dataset is clean, no missing values.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b9d51181-1e68-4df6-862f-121c09f0bc52",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "Base = Base.withColumnRenamed(\"Accor/Payoff\", \"Accor_Payoff\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b751144c-27cc-4e32-a744-db43500a8436",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "BaseE = BaseE.withColumnRenamed(\"Accor/Payoff\", \"Accor_Payoff\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "24bf4425-d849-4f0c-858e-720f6371c10a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- code_client: string (nullable = true)\n",
      " |-- TEL_MOBILE: string (nullable = true)\n",
      " |-- code_pret: string (nullable = true)\n",
      " |-- date_de_deboursement: string (nullable = true)\n",
      " |-- produit: string (nullable = true)\n",
      " |-- code_agent: string (nullable = true)\n",
      " |-- agence: string (nullable = true)\n",
      " |-- montant_demander: string (nullable = true)\n",
      " |-- montant_accorder: string (nullable = true)\n",
      " |-- duree: string (nullable = true)\n",
      " |-- valider_par: string (nullable = true)\n",
      " |-- secteur_activite: string (nullable = true)\n",
      " |-- code_secteur_du_client: string (nullable = true)\n",
      " |-- code_secteur_projet: string (nullable = true)\n",
      " |-- gouvernorat: string (nullable = true)\n",
      " |-- status_pret: string (nullable = true)\n",
      " |-- Accor_Payoff: string (nullable = true)\n",
      " |-- id: string (nullable = true)\n",
      " |-- ACCOUNT_OFFICER: string (nullable = true)\n",
      " |-- INDUSTRY: string (nullable = true)\n",
      " |-- NATIONALITY: string (nullable = true)\n",
      " |-- GENDER: string (nullable = true)\n",
      " |-- SECTOR: string (nullable = true)\n",
      " |-- MARITAL_STATUS: string (nullable = true)\n",
      " |-- LR_CARTE_BQ: string (nullable = true)\n",
      " |-- LR_CUS_MNT_REV: string (nullable = true)\n",
      " |-- SUBURB_TOWN: string (nullable = true)\n",
      " |-- AGE: string (nullable = true)\n",
      " |-- Fraude: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Schema of the data\n",
    "BaseE.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51b174ee-50e5-4302-a685-5cdbb9ca8e43",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "68be8470-354a-4139-b617-f723a01490bc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n",
      "|code_client|\n",
      "+-----------+\n",
      "|    2097563|\n",
      "|    2097615|\n",
      "|    2551368|\n",
      "|    2551451|\n",
      "|    2551527|\n",
      "|    2551726|\n",
      "|    2551726|\n",
      "|    2097834|\n",
      "|    2551802|\n",
      "|    2097847|\n",
      "|    2098019|\n",
      "|    2551915|\n",
      "|    2098089|\n",
      "|    2551922|\n",
      "|    2551922|\n",
      "|    2098089|\n",
      "|    2551963|\n",
      "|    2551992|\n",
      "|    2098328|\n",
      "|    2551992|\n",
      "+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "BaseE.select(\"code_client\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2b077135-df59-4b23-97ec-c103ebbf01a8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml import Pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "86da3759-5f6d-4123-9fa1-7a56e67ec306",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|   produit|\n",
      "+----------+\n",
      "|   Mawilni|\n",
      "|   Mawilni|\n",
      "|   Mawilni|\n",
      "|Mouasasaty|\n",
      "|   Mawilni|\n",
      "|   Mawilni|\n",
      "|   Mawilni|\n",
      "|     Darna|\n",
      "|   Mawilni|\n",
      "|     Darna|\n",
      "|    Taalim|\n",
      "|   Mawilni|\n",
      "|   Mawilni|\n",
      "|   Mawilni|\n",
      "|     Darna|\n",
      "|   Mawilni|\n",
      "|   Mawilni|\n",
      "|   Mawilni|\n",
      "|   Mawssem|\n",
      "|   Mawilni|\n",
      "+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "BaseE.select(\"produit\").show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3f2260a-3e11-48d1-a734-7eb5338e010a",
   "metadata": {
    "tags": []
   },
   "source": [
    "from pyspark.ml.feature import OneHotEncoder\n",
    "\n",
    "# Créer une instance de StringIndexer pour chaque colonne à encoder\n",
    "indexer = StringIndexer(inputCol=\"produit\", outputCol=\"produit_indexed\")\n",
    "\n",
    "# Appliquer la transformation d'indexation\n",
    "indexed = indexer.fit(BaseE).transform(BaseE)\n",
    "\n",
    "# Créer une instance de OneHotEncoder pour chaque colonne indexée\n",
    "encoder = OneHotEncoder(inputCols=[\"produit_indexed\"], outputCols=[\"produitF_encoded\"])\n",
    "\n",
    "# Appliquer la transformation d'encodage one-hot\n",
    "df_encoded = encoder.fit(indexed).transform(indexed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ca6cbdb6-40e3-4e0e-a670-878622b056e9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# pip install --quiet scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "907311a8-00c5-453c-9648-b3fa5b578968",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# from sklearn.preprocessing import LabelEncoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "29bded35-b867-47b3-abcf-d5085dff321c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8a3faa47-fa51-42c7-bb4a-96d7dfc02281",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/05/19 18:05:57 WARN DAGScheduler: Broadcasting large task binary with size 21.8 MiB\n",
      "[Stage 56:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+------------+--------------------+----------+----------+----------------+----------------+----------------+-----+--------------+-------------------+----------------------+-------------------+-----------+-----------+------------+-------+---------------+--------+-----------+------+------+--------------+-----------+--------------+-----------+---+------+-------------+-------------+----------------+--------+-------------+--------------+-------------+------------------+-------------+-------+--------+---------+----------------------+-----------+\n",
      "|code_client|TEL_MOBILE|   code_pret|date_de_deboursement|   produit|code_agent|          agence|montant_demander|montant_accorder|duree|   valider_par|   secteur_activite|code_secteur_du_client|code_secteur_projet|gouvernorat|status_pret|Accor_Payoff|     id|ACCOUNT_OFFICER|INDUSTRY|NATIONALITY|GENDER|SECTOR|MARITAL_STATUS|LR_CARTE_BQ|LR_CUS_MNT_REV|SUBURB_TOWN|AGE|Fraude|status_pret_F|LR_CARTE_BQ_F|MARITAL_STATUS_F|GENDER_F|NATIONALITY_F|Accor_Payoff_F|gouvernorat_F|secteur_activite_F|valider_par_F|duree_F|agence_F|produit_F|date_de_deboursement_F|code_pret_F|\n",
      "+-----------+----------+------------+--------------------+----------+----------+----------------+----------------+----------------+-----+--------------+-------------------+----------------------+-------------------+-----------+-----------+------------+-------+---------------+--------+-----------+------+------+--------------+-----------+--------------+-----------+---+------+-------------+-------------+----------------+--------+-------------+--------------+-------------+------------------+-------------+-------+--------+---------+----------------------+-----------+\n",
      "|    2097563|       3.0|AA2114701BWS| 2021-05-27 00:00:00|   Mawilni|      3299|         Soliman|          2500.0|        2000.000|  17M|BARGUELLIL.WA1|           Commerce|                156055|             156055|     Nabeul|      CLOSE|      Payoff|2097563|            469|    3016|         TN|FEMALE|  3000|       MARRIED|         NO|         500.0|     1560.0| 43|     0|          0.0|          0.0|             0.0|     0.0|          0.0|           1.0|          0.0|               1.0|         79.0|    5.0|    11.0|      0.0|                 656.0|   389303.0|\n",
      "|    2097615|       3.0|AA20324QYV32| 2020-11-19 00:00:00|   Mawilni|      2863|      Tajerouine|          1000.0|        2500.000|  25M|   CHERNI.AYM1|         Production|                235552|             235562|     Le Kef|      CLOSE|      Payoff|2097615|            473|    6032|         TN|FEMALE|  6000|       MARRIED|        YES|        9999.0|     2355.0| 28|     1|          0.0|          1.0|             0.0|     0.0|          0.0|           1.0|          8.0|               4.0|          0.0|    6.0|    12.0|      0.0|                 342.0|   301351.0|\n",
      "|    2551368|       3.0|AA19170RY5K0| 2019-06-19 00:00:00|   Mawilni|      2918|        Zaghouan|          2000.0|        1000.000|  10M|  ARFAOUI.KHA1|Agriculture&Elevage|                165151|             165151|   Zaghouan|      CLOSE|      Payoff|2551368|            478|    1014|         TN|  MALE|  1000|       MARRIED|         NO|        1000.0|     1651.0| 40|     0|          0.0|          0.0|             0.0|     1.0|          0.0|           1.0|         19.0|               0.0|        177.0|   19.0|    33.0|      0.0|                 138.0|    61127.0|\n",
      "|    2551451|       3.0|AA19247VC7Q7| 2019-09-05 00:00:00|Mouasasaty|      2227|         El-Krib|         25000.0|       10000.000|  28M|BOUGHANMI.KAR1|           Services|                215551|             215551|       Beja|      CLOSE|    Accorder|2551451|            418|    7005|         TN|FEMALE|  7000|       MARRIED|        YES|         600.0|     2155.0| 58|     0|          0.0|          1.0|             0.0|     0.0|          0.0|           0.0|         11.0|               3.0|        196.0|   18.0|    65.0|      4.0|                  24.0|    97264.0|\n",
      "|    2551527|       3.0|AA191707V5F8| 2019-06-20 00:00:00|   Mawilni|      2805|         Siliana|          5000.0|        5000.000|  25M|ABIDALLAH.SAB1|           Commerce|                245152|             245152|    Siliana|      CLOSE|      Payoff|2551527|            468|    3010|         TN|  MALE|  3000|       MARRIED|         NO|        1500.0|     2451.0| 42|     1|          0.0|          0.0|             0.0|     1.0|          0.0|           1.0|         16.0|               1.0|         72.0|    6.0|    30.0|      0.0|                 172.0|    60656.0|\n",
      "|    2551726|       3.0|AA191701J1N4| 2019-06-20 00:00:00|   Mawilni|      2700|      Oued Ellil|          5000.0|        2000.000|  13M|   NOUIRI.NAJ1|         Production|                145351|             145351|    Manouba|      CLOSE|      Payoff|2551726|            458|    6026|         TN|  MALE|  6000|        SINGLE|        YES|         950.0|     1453.0| 30|     0|          0.0|          1.0|             1.0|     1.0|          0.0|           1.0|          4.0|               4.0|        126.0|    0.0|    40.0|      0.0|                 172.0|    60478.0|\n",
      "|    2551726|       3.0|AA201838M00V| 2020-07-03 00:00:00|   Mawilni|      2700|      Oued Ellil|          5000.0|        4000.000|  17M|   NOUIRI.NAJ1|         Production|                145351|             145351|    Manouba|      CLOSE|    Accorder|2551726|            458|    6026|         TN|  MALE|  6000|        SINGLE|        YES|         950.0|     1453.0| 30|     0|          0.0|          1.0|             1.0|     1.0|          0.0|           0.0|          4.0|               4.0|        126.0|    5.0|    40.0|      0.0|                 816.0|   229384.0|\n",
      "|    2097834|       3.0|AA19330Q0G9T| 2019-11-28 00:00:00|     Darna|      3523|          Tozeur|           700.0|         700.000|  16M|  DAAFOUS.HAD1|              Autre|                625256|             625259|     Tozeur|      CLOSE|    Accorder|2097834|            477|    4005|         TN|FEMALE|  4000|       MARRIED|         NO|        1200.0|     6252.0| 34|     1|          0.0|          0.0|             0.0|     0.0|          0.0|           0.0|         22.0|               2.0|         80.0|    4.0|    37.0|      2.0|                 652.0|   147889.0|\n",
      "|    2551802|       3.0|AA191709DWHV| 2019-06-20 00:00:00|   Mawilni|      2114|     Bizerte Sud|          1000.0|        1000.000|  15M|  ADDASSI.SAB1|           Commerce|                175353|             175353|    Bizerte|      CLOSE|      Payoff|2551802|            507|    3013|         TN|FEMALE|  3000|       MARRIED|        YES|         650.0|     1753.0| 33|     1|          0.0|          1.0|             0.0|     0.0|          0.0|           1.0|          2.0|               1.0|          5.0|    3.0|    87.0|      0.0|                 172.0|    60703.0|\n",
      "|    2097847|       3.0|AA2005061Y6N| 2020-02-19 00:00:00|     Darna|      2704|      Oued Ellil|          3000.0|        3000.000| 652D|    RIAHI.SAW1|              Autre|                145357|             145357|    Manouba|      CLOSE|      Payoff|2097847|            458|    4005|         TN|FEMALE|  4000|       MARRIED|         NO|        1200.0|     1453.0| 62|     1|          0.0|          0.0|             0.0|     0.0|          0.0|           1.0|          4.0|               2.0|         37.0|  406.0|    40.0|      2.0|                 162.0|   187287.0|\n",
      "|    2098019|       3.0|AA220893TK2V| 2022-03-31 00:00:00|    Taalim|      3275|         Enfidha|          1500.0|        1000.000| 344D|  NOURANI.SOF1|              Autre|                316058|             316051|     Sousse|      CLOSE|    Accorder|2098019|            485|    4005|         TN|  MALE|  4000|       MARRIED|        YES|        1000.0|     3160.0| 55|     0|          0.0|          1.0|             0.0|     1.0|          0.0|           0.0|          6.0|               2.0|         69.0|   76.0|    83.0|      3.0|                 783.0|   547894.0|\n",
      "|    2551915|       3.0|AA20261NJ3B8| 2020-09-18 00:00:00|   Mawilni|      2534|Menzel Bourguiba|          2500.0|        2500.000|  13M| CHAALALI.WAJ1|           Commerce|                175856|             175856|    Bizerte|      CLOSE|    Accorder|2551915|            444|    3013|         TN|FEMALE|  3000|       MARRIED|         NO|         400.0|     1758.0| 41|     1|          0.0|          0.0|             0.0|     0.0|          0.0|           0.0|          2.0|               1.0|         34.0|    0.0|     9.0|      0.0|                  14.0|   266491.0|\n",
      "|    2098089|       3.0|AA221057YX16| 2022-04-15 00:00:00|   Mawilni|      3023|         Soliman|          5000.0|        5000.000|  19M|  YAKOUBI.WAL1|           Services|                156154|             156154|     Nabeul|    CURRENT|    Accorder|2098089|            469|    7029|         TN|  MALE|  7000|       MARRIED|        YES|         850.0|     1561.0| 48|     1|          1.0|          1.0|             0.0|     1.0|          0.0|           0.0|          0.0|               3.0|         12.0|    1.0|    11.0|      0.0|                 364.0|   555353.0|\n",
      "|    2551922|       3.0|AA22312KGXW0| 2022-11-08 00:00:00|   Mawilni|      2819|         Soliman|          1500.0|        1100.000|  13M|  YAKOUBI.WAL1|           Services|                156154|             156154|     Nabeul|    CURRENT|    Accorder|2551922|            469|    7027|         TN|  MALE|  7000|       MARRIED|         NO|        1430.0|     1561.0| 65|     0|          1.0|          0.0|             0.0|     1.0|          0.0|           0.0|          0.0|               3.0|         12.0|    0.0|    11.0|      0.0|                 280.0|   659459.0|\n",
      "|    2551922|       3.0|AA19171P7V41| 2019-06-20 00:00:00|     Darna|      2819|         Soliman|          1500.0|        1500.000|  16M|  YAKOUBI.WAL1|           Services|                156154|             156154|     Nabeul|      CLOSE|    Accorder|2551922|            469|    7027|         TN|  MALE|  7000|       MARRIED|         NO|        1430.0|     1561.0| 65|     1|          0.0|          0.0|             0.0|     1.0|          0.0|           0.0|          0.0|               3.0|         12.0|    4.0|    11.0|      2.0|                 172.0|    61898.0|\n",
      "|    2098089|       3.0|AA20343381GT| 2020-12-10 00:00:00|   Mawilni|      3023|         Soliman|          5000.0|        5000.000|  19M|  YAKOUBI.WAL1|           Services|                156154|             156154|     Nabeul|      CLOSE|      Payoff|2098089|            469|    7029|         TN|  MALE|  7000|       MARRIED|        YES|         850.0|     1561.0| 48|     1|          0.0|          1.0|             0.0|     1.0|          0.0|           1.0|          0.0|               3.0|         12.0|    1.0|    11.0|      0.0|                 401.0|   308887.0|\n",
      "|    2551963|       3.0|AA19171KV4TP| 2019-06-20 00:00:00|   Mawilni|      3268|           Gafsa|          3000.0|        3000.000|  13M|    ALOUI.AMM1|           Services|                615458|             615458|      Gafsa|      CLOSE|      Payoff|2551963|            422|    7009|         TN|FEMALE|  7000|       MARRIED|        YES|         864.0|     6154.0| 36|     1|          0.0|          1.0|             0.0|     0.0|          0.0|           1.0|         18.0|               3.0|         18.0|    0.0|    45.0|      0.0|                 172.0|    61795.0|\n",
      "|    2551992|       3.0|AA19171JVCPL| 2019-06-20 00:00:00|   Mawilni|      2087|    Béni Khalled|          3500.0|        1500.000|  10M|KHAMMASSI.SAL1|         Production|                156351|             156351|     Nabeul|      CLOSE|    Accorder|2551992|            408|    6002|         TN|  MALE|  6000|       MARRIED|         NO|        1800.0|     1563.0| 40|     0|          0.0|          0.0|             0.0|     1.0|          0.0|           0.0|          0.0|               4.0|         60.0|   19.0|    41.0|      0.0|                 172.0|    61774.0|\n",
      "|    2098328|       3.0|AA21011T4LRT| 2021-01-11 00:00:00|   Mawssem|      3620|         Sejnene|          1500.0|        3500.000|  15M|MECHERGUI.MEH1|Agriculture&Elevage|                175454|             175454|    Bizerte|      CLOSE|      Payoff|2098328|            484|    1015|         TN|FEMALE|  1000|       MARRIED|        YES|        9999.0|     1754.0| 61|     1|          0.0|          1.0|             0.0|     0.0|          0.0|           1.0|          2.0|               0.0|         35.0|    3.0|    64.0|      1.0|                 603.0|   324504.0|\n",
      "|    2551992|       3.0|AA2110246TV0| 2021-04-12 00:00:00|   Mawilni|      2087|    Béni Khalled|          3500.0|        4000.000| 476D|   AMAYLI.CHA1|         Production|                156351|             156351|     Nabeul|      CLOSE|      Payoff|2551992|            408|    6002|         TN|  MALE|  6000|       MARRIED|         NO|        1800.0|     1563.0| 40|     1|          0.0|          0.0|             0.0|     1.0|          0.0|           1.0|          0.0|               4.0|          2.0|  121.0|    41.0|      0.0|                  61.0|   370014.0|\n",
      "+-----------+----------+------------+--------------------+----------+----------+----------------+----------------+----------------+-----+--------------+-------------------+----------------------+-------------------+-----------+-----------+------------+-------+---------------+--------+-----------+------+------+--------------+-----------+--------------+-----------+---+------+-------------+-------------+----------------+--------+-------------+--------------+-------------+------------------+-------------+-------+--------+---------+----------------------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "\n",
    "columns_to_encode = [\"status_pret\", \"LR_CARTE_BQ\",\"MARITAL_STATUS\",\"GENDER\",\"NATIONALITY\",\"Accor_Payoff\",\"gouvernorat\",\"secteur_activite\",\"valider_par\",\"duree\",\"agence\",\"produit\",\"date_de_deboursement\",\"code_pret\"]\n",
    "\n",
    "# Create a StringIndexer for each column\n",
    "string_indexers = [StringIndexer(inputCol=col, outputCol=col+'_F') for col in columns_to_encode]\n",
    "\n",
    "# Fit and transform the DataFrame for each StringIndexer\n",
    "indexed_data = BaseE\n",
    "for indexer in string_indexers:\n",
    "    indexed_data = indexer.fit(indexed_data).transform(indexed_data)\n",
    "\n",
    "\n",
    "# Show the transformed DataFrame\n",
    "indexed_data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cde897c0-c457-4b95-a550-6e25a8600b57",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/05/19 18:05:59 WARN DAGScheduler: Broadcasting large task binary with size 21.8 MiB\n",
      "[Stage 57:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+----------+----------------+----------------+----------------------+-------------------+-------+---------------+--------+------+--------------+-----------+---+------+-------------+-------------+----------------+--------+-------------+--------------+-------------+------------------+-------------+-------+--------+---------+----------------------+-----------+\n",
      "|code_client|TEL_MOBILE|code_agent|montant_demander|montant_accorder|code_secteur_du_client|code_secteur_projet|     id|ACCOUNT_OFFICER|INDUSTRY|SECTOR|LR_CUS_MNT_REV|SUBURB_TOWN|AGE|Fraude|status_pret_F|LR_CARTE_BQ_F|MARITAL_STATUS_F|GENDER_F|NATIONALITY_F|Accor_Payoff_F|gouvernorat_F|secteur_activite_F|valider_par_F|duree_F|agence_F|produit_F|date_de_deboursement_F|code_pret_F|\n",
      "+-----------+----------+----------+----------------+----------------+----------------------+-------------------+-------+---------------+--------+------+--------------+-----------+---+------+-------------+-------------+----------------+--------+-------------+--------------+-------------+------------------+-------------+-------+--------+---------+----------------------+-----------+\n",
      "|    2097563|       3.0|      3299|          2500.0|        2000.000|                156055|             156055|2097563|            469|    3016|  3000|         500.0|     1560.0| 43|     0|          0.0|          0.0|             0.0|     0.0|          0.0|           1.0|          0.0|               1.0|         79.0|    5.0|    11.0|      0.0|                 656.0|   389303.0|\n",
      "|    2097615|       3.0|      2863|          1000.0|        2500.000|                235552|             235562|2097615|            473|    6032|  6000|        9999.0|     2355.0| 28|     1|          0.0|          1.0|             0.0|     0.0|          0.0|           1.0|          8.0|               4.0|          0.0|    6.0|    12.0|      0.0|                 342.0|   301351.0|\n",
      "|    2551368|       3.0|      2918|          2000.0|        1000.000|                165151|             165151|2551368|            478|    1014|  1000|        1000.0|     1651.0| 40|     0|          0.0|          0.0|             0.0|     1.0|          0.0|           1.0|         19.0|               0.0|        177.0|   19.0|    33.0|      0.0|                 138.0|    61127.0|\n",
      "|    2551451|       3.0|      2227|         25000.0|       10000.000|                215551|             215551|2551451|            418|    7005|  7000|         600.0|     2155.0| 58|     0|          0.0|          1.0|             0.0|     0.0|          0.0|           0.0|         11.0|               3.0|        196.0|   18.0|    65.0|      4.0|                  24.0|    97264.0|\n",
      "|    2551527|       3.0|      2805|          5000.0|        5000.000|                245152|             245152|2551527|            468|    3010|  3000|        1500.0|     2451.0| 42|     1|          0.0|          0.0|             0.0|     1.0|          0.0|           1.0|         16.0|               1.0|         72.0|    6.0|    30.0|      0.0|                 172.0|    60656.0|\n",
      "|    2551726|       3.0|      2700|          5000.0|        2000.000|                145351|             145351|2551726|            458|    6026|  6000|         950.0|     1453.0| 30|     0|          0.0|          1.0|             1.0|     1.0|          0.0|           1.0|          4.0|               4.0|        126.0|    0.0|    40.0|      0.0|                 172.0|    60478.0|\n",
      "|    2551726|       3.0|      2700|          5000.0|        4000.000|                145351|             145351|2551726|            458|    6026|  6000|         950.0|     1453.0| 30|     0|          0.0|          1.0|             1.0|     1.0|          0.0|           0.0|          4.0|               4.0|        126.0|    5.0|    40.0|      0.0|                 816.0|   229384.0|\n",
      "|    2097834|       3.0|      3523|           700.0|         700.000|                625256|             625259|2097834|            477|    4005|  4000|        1200.0|     6252.0| 34|     1|          0.0|          0.0|             0.0|     0.0|          0.0|           0.0|         22.0|               2.0|         80.0|    4.0|    37.0|      2.0|                 652.0|   147889.0|\n",
      "|    2551802|       3.0|      2114|          1000.0|        1000.000|                175353|             175353|2551802|            507|    3013|  3000|         650.0|     1753.0| 33|     1|          0.0|          1.0|             0.0|     0.0|          0.0|           1.0|          2.0|               1.0|          5.0|    3.0|    87.0|      0.0|                 172.0|    60703.0|\n",
      "|    2097847|       3.0|      2704|          3000.0|        3000.000|                145357|             145357|2097847|            458|    4005|  4000|        1200.0|     1453.0| 62|     1|          0.0|          0.0|             0.0|     0.0|          0.0|           1.0|          4.0|               2.0|         37.0|  406.0|    40.0|      2.0|                 162.0|   187287.0|\n",
      "|    2098019|       3.0|      3275|          1500.0|        1000.000|                316058|             316051|2098019|            485|    4005|  4000|        1000.0|     3160.0| 55|     0|          0.0|          1.0|             0.0|     1.0|          0.0|           0.0|          6.0|               2.0|         69.0|   76.0|    83.0|      3.0|                 783.0|   547894.0|\n",
      "|    2551915|       3.0|      2534|          2500.0|        2500.000|                175856|             175856|2551915|            444|    3013|  3000|         400.0|     1758.0| 41|     1|          0.0|          0.0|             0.0|     0.0|          0.0|           0.0|          2.0|               1.0|         34.0|    0.0|     9.0|      0.0|                  14.0|   266491.0|\n",
      "|    2098089|       3.0|      3023|          5000.0|        5000.000|                156154|             156154|2098089|            469|    7029|  7000|         850.0|     1561.0| 48|     1|          1.0|          1.0|             0.0|     1.0|          0.0|           0.0|          0.0|               3.0|         12.0|    1.0|    11.0|      0.0|                 364.0|   555353.0|\n",
      "|    2551922|       3.0|      2819|          1500.0|        1100.000|                156154|             156154|2551922|            469|    7027|  7000|        1430.0|     1561.0| 65|     0|          1.0|          0.0|             0.0|     1.0|          0.0|           0.0|          0.0|               3.0|         12.0|    0.0|    11.0|      0.0|                 280.0|   659459.0|\n",
      "|    2551922|       3.0|      2819|          1500.0|        1500.000|                156154|             156154|2551922|            469|    7027|  7000|        1430.0|     1561.0| 65|     1|          0.0|          0.0|             0.0|     1.0|          0.0|           0.0|          0.0|               3.0|         12.0|    4.0|    11.0|      2.0|                 172.0|    61898.0|\n",
      "|    2098089|       3.0|      3023|          5000.0|        5000.000|                156154|             156154|2098089|            469|    7029|  7000|         850.0|     1561.0| 48|     1|          0.0|          1.0|             0.0|     1.0|          0.0|           1.0|          0.0|               3.0|         12.0|    1.0|    11.0|      0.0|                 401.0|   308887.0|\n",
      "|    2551963|       3.0|      3268|          3000.0|        3000.000|                615458|             615458|2551963|            422|    7009|  7000|         864.0|     6154.0| 36|     1|          0.0|          1.0|             0.0|     0.0|          0.0|           1.0|         18.0|               3.0|         18.0|    0.0|    45.0|      0.0|                 172.0|    61795.0|\n",
      "|    2551992|       3.0|      2087|          3500.0|        1500.000|                156351|             156351|2551992|            408|    6002|  6000|        1800.0|     1563.0| 40|     0|          0.0|          0.0|             0.0|     1.0|          0.0|           0.0|          0.0|               4.0|         60.0|   19.0|    41.0|      0.0|                 172.0|    61774.0|\n",
      "|    2098328|       3.0|      3620|          1500.0|        3500.000|                175454|             175454|2098328|            484|    1015|  1000|        9999.0|     1754.0| 61|     1|          0.0|          1.0|             0.0|     0.0|          0.0|           1.0|          2.0|               0.0|         35.0|    3.0|    64.0|      1.0|                 603.0|   324504.0|\n",
      "|    2551992|       3.0|      2087|          3500.0|        4000.000|                156351|             156351|2551992|            408|    6002|  6000|        1800.0|     1563.0| 40|     1|          0.0|          0.0|             0.0|     1.0|          0.0|           1.0|          0.0|               4.0|          2.0|  121.0|    41.0|      0.0|                  61.0|   370014.0|\n",
      "+-----------+----------+----------+----------------+----------------+----------------------+-------------------+-------+---------------+--------+------+--------------+-----------+---+------+-------------+-------------+----------------+--------+-------------+--------------+-------------+------------------+-------------+-------+--------+---------+----------------------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Supprimer les colonnes d'origine de indexed_data\n",
    "indexed_data = indexed_data.drop(*columns_to_encode)\n",
    "\n",
    "# Afficher le DataFrame résultant\n",
    "indexed_data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e1365344-df87-48e4-aaa9-9fb537d112af",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|produit_F|\n",
      "+---------+\n",
      "|      0.0|\n",
      "|      0.0|\n",
      "|      0.0|\n",
      "|      4.0|\n",
      "|      0.0|\n",
      "|      0.0|\n",
      "|      0.0|\n",
      "|      2.0|\n",
      "|      0.0|\n",
      "|      2.0|\n",
      "|      3.0|\n",
      "|      0.0|\n",
      "|      0.0|\n",
      "|      0.0|\n",
      "|      2.0|\n",
      "|      0.0|\n",
      "|      0.0|\n",
      "|      0.0|\n",
      "|      1.0|\n",
      "|      0.0|\n",
      "+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "indexed_data.select(\"produit_F\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f19ab9b5-65f6-478d-a522-03693167f62d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('code_client', 'string'),\n",
       " ('TEL_MOBILE', 'string'),\n",
       " ('code_agent', 'string'),\n",
       " ('montant_demander', 'string'),\n",
       " ('montant_accorder', 'string'),\n",
       " ('code_secteur_du_client', 'string'),\n",
       " ('code_secteur_projet', 'string'),\n",
       " ('id', 'string'),\n",
       " ('ACCOUNT_OFFICER', 'string'),\n",
       " ('INDUSTRY', 'string'),\n",
       " ('SECTOR', 'string'),\n",
       " ('LR_CUS_MNT_REV', 'string'),\n",
       " ('SUBURB_TOWN', 'string'),\n",
       " ('AGE', 'string'),\n",
       " ('Fraude', 'string'),\n",
       " ('status_pret_F', 'double'),\n",
       " ('LR_CARTE_BQ_F', 'double'),\n",
       " ('MARITAL_STATUS_F', 'double'),\n",
       " ('GENDER_F', 'double'),\n",
       " ('NATIONALITY_F', 'double'),\n",
       " ('Accor_Payoff_F', 'double'),\n",
       " ('gouvernorat_F', 'double'),\n",
       " ('secteur_activite_F', 'double'),\n",
       " ('valider_par_F', 'double'),\n",
       " ('duree_F', 'double'),\n",
       " ('agence_F', 'double'),\n",
       " ('produit_F', 'double'),\n",
       " ('date_de_deboursement_F', 'double'),\n",
       " ('code_pret_F', 'double')]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indexed_data.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d4863dd1-e5d4-4a50-8eb2-baed092f257a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import DoubleType\n",
    "\n",
    "indexed_data = indexed_data.withColumn(\"code_client\", indexed_data.code_client.cast(DoubleType()))\n",
    "indexed_data = indexed_data.withColumn(\"TEL_MOBILE\", indexed_data.TEL_MOBILE.cast(DoubleType()))\n",
    "indexed_data = indexed_data.withColumn(\"code_agent\", indexed_data.code_agent.cast(DoubleType()))\n",
    "indexed_data = indexed_data.withColumn(\"Fraude\", indexed_data.Fraude.cast('int'))\n",
    "indexed_data= indexed_data.withColumn(\"montant_demander\", indexed_data.montant_demander.cast(DoubleType()))\n",
    "indexed_data = indexed_data.withColumn(\"montant_accorder\", indexed_data.montant_accorder.cast(DoubleType()))\n",
    "indexed_data = indexed_data.withColumn(\"code_secteur_du_client\", indexed_data.code_secteur_du_client.cast(DoubleType()))\n",
    "indexed_data = indexed_data.withColumn(\"code_secteur_projet\", indexed_data.code_secteur_projet.cast(DoubleType()))\n",
    "indexed_data = indexed_data.withColumn(\"id\", indexed_data.id.cast(DoubleType()))\n",
    "indexed_data = indexed_data.withColumn(\"ACCOUNT_OFFICER\", indexed_data.ACCOUNT_OFFICER.cast(DoubleType()))\n",
    "indexed_data = indexed_data.withColumn(\"INDUSTRY\",indexed_data.INDUSTRY.cast(DoubleType()))\n",
    "indexed_data = indexed_data.withColumn(\"SECTOR\", indexed_data.SECTOR.cast(DoubleType()))\n",
    "indexed_data = indexed_data.withColumn(\"LR_CUS_MNT_REV\", indexed_data.LR_CUS_MNT_REV.cast(DoubleType()))\n",
    "indexed_data = indexed_data.withColumn(\"SUBURB_TOWN\", indexed_data.SUBURB_TOWN.cast(DoubleType()))\n",
    "indexed_data = indexed_data.withColumn(\"AGE\", indexed_data.AGE.cast('int'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "668fb081-bf4a-46d5-ac10-dbeed3e8d5c1",
   "metadata": {},
   "source": [
    "columns_to_encode = [\"status_pret\", \"LR_CARTE_BQ\",\"MARITAL_STATUS\",\"GENDER\",\"NATIONALITY\",\"Accor_Payoff\",\"gouvernorat\",\"secteur_activite\",\"valider_par\",\"duree\",\"agence\",\"produit\",\"date_de_deboursement\",\"code_pret\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ec55a6ba-1423-4511-a6b7-4845621e5c2a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('code_client', 'double'),\n",
       " ('TEL_MOBILE', 'double'),\n",
       " ('code_agent', 'double'),\n",
       " ('montant_demander', 'double'),\n",
       " ('montant_accorder', 'double'),\n",
       " ('code_secteur_du_client', 'double'),\n",
       " ('code_secteur_projet', 'double'),\n",
       " ('id', 'double'),\n",
       " ('ACCOUNT_OFFICER', 'double'),\n",
       " ('INDUSTRY', 'double'),\n",
       " ('SECTOR', 'double'),\n",
       " ('LR_CUS_MNT_REV', 'double'),\n",
       " ('SUBURB_TOWN', 'double'),\n",
       " ('AGE', 'int'),\n",
       " ('Fraude', 'int'),\n",
       " ('status_pret_F', 'double'),\n",
       " ('LR_CARTE_BQ_F', 'double'),\n",
       " ('MARITAL_STATUS_F', 'double'),\n",
       " ('GENDER_F', 'double'),\n",
       " ('NATIONALITY_F', 'double'),\n",
       " ('Accor_Payoff_F', 'double'),\n",
       " ('gouvernorat_F', 'double'),\n",
       " ('secteur_activite_F', 'double'),\n",
       " ('valider_par_F', 'double'),\n",
       " ('duree_F', 'double'),\n",
       " ('agence_F', 'double'),\n",
       " ('produit_F', 'double'),\n",
       " ('date_de_deboursement_F', 'double'),\n",
       " ('code_pret_F', 'double')]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check column data types\n",
    "indexed_data.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "49140dfa-fe1f-4f70-86d5-c68036806d91",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/05/19 18:06:15 WARN DAGScheduler: Broadcasting large task binary with size 21.8 MiB\n",
      "[Stage 59:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+----------+----------------+----------------+----------------------+-------------------+---------+---------------+--------+------+--------------+-----------+---+------+-------------+-------------+----------------+--------+-------------+--------------+-------------+------------------+-------------+-------+--------+---------+----------------------+-----------+\n",
      "|code_client|TEL_MOBILE|code_agent|montant_demander|montant_accorder|code_secteur_du_client|code_secteur_projet|       id|ACCOUNT_OFFICER|INDUSTRY|SECTOR|LR_CUS_MNT_REV|SUBURB_TOWN|AGE|Fraude|status_pret_F|LR_CARTE_BQ_F|MARITAL_STATUS_F|GENDER_F|NATIONALITY_F|Accor_Payoff_F|gouvernorat_F|secteur_activite_F|valider_par_F|duree_F|agence_F|produit_F|date_de_deboursement_F|code_pret_F|\n",
      "+-----------+----------+----------+----------------+----------------+----------------------+-------------------+---------+---------------+--------+------+--------------+-----------+---+------+-------------+-------------+----------------+--------+-------------+--------------+-------------+------------------+-------------+-------+--------+---------+----------------------+-----------+\n",
      "|  2097563.0|       3.0|    3299.0|          2500.0|          2000.0|              156055.0|           156055.0|2097563.0|          469.0|  3016.0|3000.0|         500.0|     1560.0| 43|     0|          0.0|          0.0|             0.0|     0.0|          0.0|           1.0|          0.0|               1.0|         79.0|    5.0|    11.0|      0.0|                 656.0|   389303.0|\n",
      "|  2097615.0|       3.0|    2863.0|          1000.0|          2500.0|              235552.0|           235562.0|2097615.0|          473.0|  6032.0|6000.0|        9999.0|     2355.0| 28|     1|          0.0|          1.0|             0.0|     0.0|          0.0|           1.0|          8.0|               4.0|          0.0|    6.0|    12.0|      0.0|                 342.0|   301351.0|\n",
      "|  2551368.0|       3.0|    2918.0|          2000.0|          1000.0|              165151.0|           165151.0|2551368.0|          478.0|  1014.0|1000.0|        1000.0|     1651.0| 40|     0|          0.0|          0.0|             0.0|     1.0|          0.0|           1.0|         19.0|               0.0|        177.0|   19.0|    33.0|      0.0|                 138.0|    61127.0|\n",
      "|  2551451.0|       3.0|    2227.0|         25000.0|         10000.0|              215551.0|           215551.0|2551451.0|          418.0|  7005.0|7000.0|         600.0|     2155.0| 58|     0|          0.0|          1.0|             0.0|     0.0|          0.0|           0.0|         11.0|               3.0|        196.0|   18.0|    65.0|      4.0|                  24.0|    97264.0|\n",
      "|  2551527.0|       3.0|    2805.0|          5000.0|          5000.0|              245152.0|           245152.0|2551527.0|          468.0|  3010.0|3000.0|        1500.0|     2451.0| 42|     1|          0.0|          0.0|             0.0|     1.0|          0.0|           1.0|         16.0|               1.0|         72.0|    6.0|    30.0|      0.0|                 172.0|    60656.0|\n",
      "|  2551726.0|       3.0|    2700.0|          5000.0|          2000.0|              145351.0|           145351.0|2551726.0|          458.0|  6026.0|6000.0|         950.0|     1453.0| 30|     0|          0.0|          1.0|             1.0|     1.0|          0.0|           1.0|          4.0|               4.0|        126.0|    0.0|    40.0|      0.0|                 172.0|    60478.0|\n",
      "|  2551726.0|       3.0|    2700.0|          5000.0|          4000.0|              145351.0|           145351.0|2551726.0|          458.0|  6026.0|6000.0|         950.0|     1453.0| 30|     0|          0.0|          1.0|             1.0|     1.0|          0.0|           0.0|          4.0|               4.0|        126.0|    5.0|    40.0|      0.0|                 816.0|   229384.0|\n",
      "|  2097834.0|       3.0|    3523.0|           700.0|           700.0|              625256.0|           625259.0|2097834.0|          477.0|  4005.0|4000.0|        1200.0|     6252.0| 34|     1|          0.0|          0.0|             0.0|     0.0|          0.0|           0.0|         22.0|               2.0|         80.0|    4.0|    37.0|      2.0|                 652.0|   147889.0|\n",
      "|  2551802.0|       3.0|    2114.0|          1000.0|          1000.0|              175353.0|           175353.0|2551802.0|          507.0|  3013.0|3000.0|         650.0|     1753.0| 33|     1|          0.0|          1.0|             0.0|     0.0|          0.0|           1.0|          2.0|               1.0|          5.0|    3.0|    87.0|      0.0|                 172.0|    60703.0|\n",
      "|  2097847.0|       3.0|    2704.0|          3000.0|          3000.0|              145357.0|           145357.0|2097847.0|          458.0|  4005.0|4000.0|        1200.0|     1453.0| 62|     1|          0.0|          0.0|             0.0|     0.0|          0.0|           1.0|          4.0|               2.0|         37.0|  406.0|    40.0|      2.0|                 162.0|   187287.0|\n",
      "|  2098019.0|       3.0|    3275.0|          1500.0|          1000.0|              316058.0|           316051.0|2098019.0|          485.0|  4005.0|4000.0|        1000.0|     3160.0| 55|     0|          0.0|          1.0|             0.0|     1.0|          0.0|           0.0|          6.0|               2.0|         69.0|   76.0|    83.0|      3.0|                 783.0|   547894.0|\n",
      "|  2551915.0|       3.0|    2534.0|          2500.0|          2500.0|              175856.0|           175856.0|2551915.0|          444.0|  3013.0|3000.0|         400.0|     1758.0| 41|     1|          0.0|          0.0|             0.0|     0.0|          0.0|           0.0|          2.0|               1.0|         34.0|    0.0|     9.0|      0.0|                  14.0|   266491.0|\n",
      "|  2098089.0|       3.0|    3023.0|          5000.0|          5000.0|              156154.0|           156154.0|2098089.0|          469.0|  7029.0|7000.0|         850.0|     1561.0| 48|     1|          1.0|          1.0|             0.0|     1.0|          0.0|           0.0|          0.0|               3.0|         12.0|    1.0|    11.0|      0.0|                 364.0|   555353.0|\n",
      "|  2551922.0|       3.0|    2819.0|          1500.0|          1100.0|              156154.0|           156154.0|2551922.0|          469.0|  7027.0|7000.0|        1430.0|     1561.0| 65|     0|          1.0|          0.0|             0.0|     1.0|          0.0|           0.0|          0.0|               3.0|         12.0|    0.0|    11.0|      0.0|                 280.0|   659459.0|\n",
      "|  2551922.0|       3.0|    2819.0|          1500.0|          1500.0|              156154.0|           156154.0|2551922.0|          469.0|  7027.0|7000.0|        1430.0|     1561.0| 65|     1|          0.0|          0.0|             0.0|     1.0|          0.0|           0.0|          0.0|               3.0|         12.0|    4.0|    11.0|      2.0|                 172.0|    61898.0|\n",
      "|  2098089.0|       3.0|    3023.0|          5000.0|          5000.0|              156154.0|           156154.0|2098089.0|          469.0|  7029.0|7000.0|         850.0|     1561.0| 48|     1|          0.0|          1.0|             0.0|     1.0|          0.0|           1.0|          0.0|               3.0|         12.0|    1.0|    11.0|      0.0|                 401.0|   308887.0|\n",
      "|  2551963.0|       3.0|    3268.0|          3000.0|          3000.0|              615458.0|           615458.0|2551963.0|          422.0|  7009.0|7000.0|         864.0|     6154.0| 36|     1|          0.0|          1.0|             0.0|     0.0|          0.0|           1.0|         18.0|               3.0|         18.0|    0.0|    45.0|      0.0|                 172.0|    61795.0|\n",
      "|  2551992.0|       3.0|    2087.0|          3500.0|          1500.0|              156351.0|           156351.0|2551992.0|          408.0|  6002.0|6000.0|        1800.0|     1563.0| 40|     0|          0.0|          0.0|             0.0|     1.0|          0.0|           0.0|          0.0|               4.0|         60.0|   19.0|    41.0|      0.0|                 172.0|    61774.0|\n",
      "|  2098328.0|       3.0|    3620.0|          1500.0|          3500.0|              175454.0|           175454.0|2098328.0|          484.0|  1015.0|1000.0|        9999.0|     1754.0| 61|     1|          0.0|          1.0|             0.0|     0.0|          0.0|           1.0|          2.0|               0.0|         35.0|    3.0|    64.0|      1.0|                 603.0|   324504.0|\n",
      "|  2551992.0|       3.0|    2087.0|          3500.0|          4000.0|              156351.0|           156351.0|2551992.0|          408.0|  6002.0|6000.0|        1800.0|     1563.0| 40|     1|          0.0|          0.0|             0.0|     1.0|          0.0|           1.0|          0.0|               4.0|          2.0|  121.0|    41.0|      0.0|                  61.0|   370014.0|\n",
      "+-----------+----------+----------+----------------+----------------+----------------------+-------------------+---------+---------------+--------+------+--------------+-----------+---+------+-------------+-------------+----------------+--------+-------------+--------------+-------------+------------------+-------------+-------+--------+---------+----------------------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "indexed_data.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cb15e03-1661-4f6c-b85d-9a7f9b9a3dab",
   "metadata": {},
   "source": [
    "EDA : Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "11b0a5e3-9660-4a23-b0f8-0acf5bfe5f90",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+\n",
      "|Fraude| count|\n",
      "+------+------+\n",
      "|     1|393096|\n",
      "|     0|293462|\n",
      "+------+------+\n",
      "\n"
     ]
    },
    {
     "ename": "AnalysisException",
     "evalue": "[PATH_ALREADY_EXISTS] Path file:/data/indexed.csv already exists. Set mode as \"overwrite\" to overwrite the existing path.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m indexed_data\u001b[38;5;241m.\u001b[39mgroupBy(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFraude\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mcount()\u001b[38;5;241m.\u001b[39mshow()\n\u001b[0;32m----> 2\u001b[0m \u001b[43mindexed_data\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moption\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mheader\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcsv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mindexed.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/pyspark/sql/readwriter.py:1799\u001b[0m, in \u001b[0;36mDataFrameWriter.csv\u001b[0;34m(self, path, mode, compression, sep, quote, escape, header, nullValue, escapeQuotes, quoteAll, dateFormat, timestampFormat, ignoreLeadingWhiteSpace, ignoreTrailingWhiteSpace, charToEscapeQuoteEscaping, encoding, emptyValue, lineSep)\u001b[0m\n\u001b[1;32m   1780\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmode(mode)\n\u001b[1;32m   1781\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_opts(\n\u001b[1;32m   1782\u001b[0m     compression\u001b[38;5;241m=\u001b[39mcompression,\n\u001b[1;32m   1783\u001b[0m     sep\u001b[38;5;241m=\u001b[39msep,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1797\u001b[0m     lineSep\u001b[38;5;241m=\u001b[39mlineSep,\n\u001b[1;32m   1798\u001b[0m )\n\u001b[0;32m-> 1799\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jwrite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcsv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/pyspark/errors/exceptions/captured.py:175\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    171\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    172\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    173\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    174\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 175\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    176\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    177\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: [PATH_ALREADY_EXISTS] Path file:/data/indexed.csv already exists. Set mode as \"overwrite\" to overwrite the existing path."
     ]
    }
   ],
   "source": [
    "indexed_data.groupBy(\"Fraude\").count().show()\n",
    "indexed_data.write.option(\"header\",True) \\\n",
    " .csv(\"indexed.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4c14090f-5688-466c-9e96-6860d70ef738",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 66:>                                                         (0 + 4) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pourcentage de fraude:  57.25605120033559\n",
      "Pourcentage de non-fraude:  42.74394879966441\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "\n",
    "from pyspark.sql.functions import expr\n",
    "\n",
    "# Créer une session Spark\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "# Supposons que vous ayez un DataFrame appelé \"data\" contenant les données\n",
    "# avec une colonne \"Fraude\" représentant les fraudes (0 pour non fraude, 1 pour fraude)\n",
    "# et d'autres colonnes si nécessaire.\n",
    "\n",
    "# Calculer le nombre total d'enregistrements\n",
    "total_count = indexed_data.count()\n",
    "\n",
    "# Calculer le nombre de fraudes\n",
    "fraude_count = indexed_data.filter(col(\"Fraude\") == 1).count()\n",
    "\n",
    "# Calculer le nombre de non-fraudes\n",
    "non_fraude_count = total_count - fraude_count\n",
    "\n",
    "# Calculer le pourcentage de fraude et de non-fraude\n",
    "pourcentage_fraude = (fraude_count / total_count) * 100\n",
    "pourcentage_non_fraude = (non_fraude_count / total_count) * 100\n",
    "\n",
    "# Afficher les résultats\n",
    "print(\"Pourcentage de fraude: \", pourcentage_fraude)\n",
    "print(\"Pourcentage de non-fraude: \", pourcentage_non_fraude)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b045f3e6-0d90-45d2-a865-de85a764fa83",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'matplotlib'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[29], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Créer les données pour le graphique\u001b[39;00m\n\u001b[1;32m      4\u001b[0m labels \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFraude\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNon-Fraude\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'matplotlib'"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Créer les données pour le graphique\n",
    "labels = ['Fraude', 'Non-Fraude']\n",
    "pourcentages = [pourcentage_fraude, pourcentage_non_fraude]\n",
    "\n",
    "# Créer le graphique à secteurs (pie chart)\n",
    "plt.pie(pourcentages, labels=labels, autopct='%1.1f%%')\n",
    "\n",
    "# Ajouter un titre\n",
    "plt.title('Pourcentage de Fraude et de Non-Fraude')\n",
    "\n",
    "# Afficher le graphique\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "cfb7c7f0-8780-4196-80f9-be9a67d0a69e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "81c9e888-7889-409e-b010-12bc7cca927d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('code_client', 'double'),\n",
       " ('TEL_MOBILE', 'double'),\n",
       " ('code_agent', 'double'),\n",
       " ('montant_demander', 'double'),\n",
       " ('montant_accorder', 'double'),\n",
       " ('code_secteur_du_client', 'double'),\n",
       " ('code_secteur_projet', 'double'),\n",
       " ('id', 'double'),\n",
       " ('ACCOUNT_OFFICER', 'double'),\n",
       " ('INDUSTRY', 'double'),\n",
       " ('SECTOR', 'double'),\n",
       " ('LR_CUS_MNT_REV', 'double'),\n",
       " ('SUBURB_TOWN', 'double'),\n",
       " ('AGE', 'int'),\n",
       " ('Fraude', 'int'),\n",
       " ('status_pret_F', 'double'),\n",
       " ('LR_CARTE_BQ_F', 'double'),\n",
       " ('MARITAL_STATUS_F', 'double'),\n",
       " ('GENDER_F', 'double'),\n",
       " ('NATIONALITY_F', 'double'),\n",
       " ('Accor_Payoff_F', 'double'),\n",
       " ('gouvernorat_F', 'double'),\n",
       " ('secteur_activite_F', 'double'),\n",
       " ('valider_par_F', 'double'),\n",
       " ('duree_F', 'double'),\n",
       " ('agence_F', 'double'),\n",
       " ('produit_F', 'double'),\n",
       " ('date_de_deboursement_F', 'double'),\n",
       " ('code_pret_F', 'double')]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indexed_data.dtypes"
   ]
  },
  {
   "cell_type": "raw",
   "id": "615f40d6-09f4-463d-99c4-a11847fb1f1f",
   "metadata": {
    "tags": []
   },
   "source": [
    "import pandas as pd\n",
    "from pandas.plotting import scatter_matrix\n",
    "\n",
    "numeric_features = ['code_client', 'code_pret_F',\n",
    "                  'produit_F', \n",
    "                    'agence_F',\n",
    "                    'duree_F' ,'status_pret_F', 'GENDER_F', \n",
    "                    'MARITAL_STATUS_F','LR_CARTE_BQ_F', \n",
    "                    'AGE']\n",
    "\n",
    "numeric_data = indexed_data.select(numeric_features).toPandas()\n",
    "axs = pd.plotting.scatter_matrix(numeric_data, figsize=(10,10));\n",
    "n = len(numeric_data.columns)\n",
    "for i in range(n):\n",
    "    v = axs[i, 0]\n",
    "    v.yaxis.label.set_rotation(0)\n",
    "    v.yaxis.label.set_ha('right')\n",
    "    v.set_yticks(())\n",
    "    h = axs[n-1, i]\n",
    "    h.xaxis.label.set_rotation(90)\n",
    "    h.set_xticks(())"
   ]
  },
  {
   "cell_type": "raw",
   "id": "bd16da96-32c4-46b5-a05f-f7cf34b495c0",
   "metadata": {},
   "source": [
    "numeric_features = ['code_client', 'TEL_MOBILE', 'code_pret_F',\n",
    "                    'date_de_deboursement_F','produit_F', \n",
    "                    'agence_F','montant_demander', 'montant_accorder',\n",
    "                    'duree_F','valider_par_F', \n",
    "                    'secteur_activite_F','code_secteur_du_client', 'code_secteur_projet', 'gouvernorat_F',\n",
    "                    'status_pret_F','Accor_Payoff_F', \n",
    "                    'id','ACCOUNT_OFFICER', 'INDUSTRY', 'NATIONALITY_F',\n",
    "                    'GENDER_F','SECTOR', \n",
    "                    'MARITAL_STATUS_F','LR_CARTE_BQ_F', 'LR_CUS_MNT_REV', 'SUBURB_TOWN',\n",
    "                    'AGE']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "28cbf822-4360-4cce-b4e2-faefa75ef2ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "assembler = VectorAssembler(inputCols=['code_client', 'TEL_MOBILE', 'code_pret_F',\n",
    "                    'date_de_deboursement_F','produit_F', \n",
    "                    'agence_F','montant_demander', 'montant_accorder',\n",
    "                    'duree_F','valider_par_F', \n",
    "                    'secteur_activite_F','code_secteur_du_client', 'code_secteur_projet', 'gouvernorat_F',\n",
    "                    'status_pret_F','Accor_Payoff_F', \n",
    "                    'id','ACCOUNT_OFFICER', 'INDUSTRY', 'NATIONALITY_F',\n",
    "                    'GENDER_F','SECTOR', \n",
    "                    'MARITAL_STATUS_F','LR_CARTE_BQ_F', 'LR_CUS_MNT_REV', 'SUBURB_TOWN',\n",
    "                    'AGE'], outputCol='features')\n",
    "data = assembler.transform(indexed_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1a19953a-584d-4b44-a4ec-87abd1d3efb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/05/19 18:07:47 WARN DAGScheduler: Broadcasting large task binary with size 21.8 MiB\n",
      "[Stage 69:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------+\n",
      "|            features|Fraude|\n",
      "+--------------------+------+\n",
      "|[2097563.0,3.0,38...|     0|\n",
      "|[2097615.0,3.0,30...|     1|\n",
      "|[2551368.0,3.0,61...|     0|\n",
      "|[2551451.0,3.0,97...|     0|\n",
      "|[2551527.0,3.0,60...|     1|\n",
      "|[2551726.0,3.0,60...|     0|\n",
      "|[2551726.0,3.0,22...|     0|\n",
      "|[2097834.0,3.0,14...|     1|\n",
      "|[2551802.0,3.0,60...|     1|\n",
      "|[2097847.0,3.0,18...|     1|\n",
      "|[2098019.0,3.0,54...|     0|\n",
      "|[2551915.0,3.0,26...|     1|\n",
      "|[2098089.0,3.0,55...|     1|\n",
      "|[2551922.0,3.0,65...|     0|\n",
      "|[2551922.0,3.0,61...|     1|\n",
      "|[2098089.0,3.0,30...|     1|\n",
      "|[2551963.0,3.0,61...|     1|\n",
      "|[2551992.0,3.0,61...|     0|\n",
      "|[2098328.0,3.0,32...|     1|\n",
      "|[2551992.0,3.0,37...|     1|\n",
      "|[2098342.0,3.0,14...|     1|\n",
      "|[2552033.0,3.0,61...|     0|\n",
      "|[2098438.0,3.0,49...|     1|\n",
      "|[2098438.0,3.0,30...|     1|\n",
      "|[2552156.0,3.0,62...|     0|\n",
      "|[2098438.0,3.0,13...|     1|\n",
      "|[2552184.0,3.0,62...|     1|\n",
      "|[2098540.0,3.0,27...|     0|\n",
      "|[2552184.0,3.0,29...|     0|\n",
      "|[2098540.0,3.0,51...|     0|\n",
      "|[2098903.0,3.0,17...|     1|\n",
      "|[2552255.0,3.0,61...|     0|\n",
      "|[2552255.0,3.0,27...|     0|\n",
      "|[2552271.0,3.0,64...|     1|\n",
      "|[2098915.0,3.0,57...|     0|\n",
      "|[2552329.0,3.0,61...|     1|\n",
      "|[2099061.0,3.0,45...|     1|\n",
      "|[2552362.0,3.0,62...|     0|\n",
      "|[2099061.0,3.0,15...|     1|\n",
      "|[2552381.0,3.0,61...|     0|\n",
      "|[2552415.0,3.0,24...|     1|\n",
      "|[2099127.0,3.0,46...|     0|\n",
      "|[2552415.0,3.0,61...|     1|\n",
      "|[2099172.0,3.0,48...|     1|\n",
      "|[2552484.0,3.0,61...|     0|\n",
      "|[2099293.0,3.0,13...|     1|\n",
      "|[2099348.0,3.0,27...|     1|\n",
      "|[2099348.0,3.0,40...|     1|\n",
      "|[2099350.0,3.0,66...|     0|\n",
      "|[2552539.0,3.0,21...|     1|\n",
      "|[2552539.0,3.0,62...|     0|\n",
      "|[2099377.0,3.0,18...|     1|\n",
      "|[2552539.0,3.0,45...|     0|\n",
      "|[2099554.0,3.0,45...|     1|\n",
      "|[2552698.0,3.0,62...|     0|\n",
      "|[2099554.0,3.0,63...|     1|\n",
      "|[2099815.0,3.0,16...|     1|\n",
      "|[2552805.0,3.0,31...|     0|\n",
      "|[2552805.0,3.0,67...|     0|\n",
      "|[2099999.0,3.0,55...|     1|\n",
      "|[2552820.0,3.0,21...|     0|\n",
      "|[2552830.0,3.0,62...|     0|\n",
      "|[2100131.0,3.0,16...|     1|\n",
      "|[2552843.0,3.0,54...|     1|\n",
      "|[2100131.0,3.0,57...|     1|\n",
      "|[2552843.0,3.0,67...|     0|\n",
      "|[2552843.0,3.0,23...|     1|\n",
      "|[2100188.0,3.0,13...|     1|\n",
      "|[2100188.0,3.0,30...|     1|\n",
      "|[2552857.0,3.0,23...|     1|\n",
      "|[2100189.0,3.0,14...|     0|\n",
      "|[2552872.0,3.0,64...|     0|\n",
      "|[2100264.0,3.0,30...|     1|\n",
      "|[2100264.0,3.0,12...|     1|\n",
      "|[2552898.0,3.0,24...|     1|\n",
      "|[2100312.0,3.0,15...|     1|\n",
      "|[2552898.0,3.0,62...|     1|\n",
      "|[2100312.0,3.0,36...|     1|\n",
      "|[2552915.0,3.0,62...|     0|\n",
      "|[2100568.0,3.0,28...|     1|\n",
      "|[2552921.0,3.0,64...|     1|\n",
      "|[2100819.0,3.0,37...|     0|\n",
      "|[2100873.0,3.0,14...|     0|\n",
      "|[2552963.0,3.0,22...|     1|\n",
      "|[2553042.0,3.0,28...|     0|\n",
      "|[2101005.0,3.0,52...|     1|\n",
      "|[2553042.0,3.0,65...|     0|\n",
      "|[2101008.0,3.0,65...|     1|\n",
      "|[2553042.0,3.0,45...|     0|\n",
      "|[2101008.0,3.0,37...|     0|\n",
      "|[2553047.0,3.0,72...|     0|\n",
      "|[2553139.0,3.0,63...|     0|\n",
      "|[2553190.0,3.0,25...|     1|\n",
      "|[2101146.0,3.0,19...|     0|\n",
      "|[2553190.0,3.0,34...|     1|\n",
      "|[2553273.0,3.0,65...|     1|\n",
      "|[2101166.0,3.0,29...|     1|\n",
      "|[2101395.0,3.0,35...|     1|\n",
      "|[2553364.0,3.0,63...|     0|\n",
      "|[2101395.0,3.0,13...|     1|\n",
      "+--------------------+------+\n",
      "only showing top 100 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "data['features','Fraude'].show(100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "21f20c02-80eb-4aa2-b4c1-0389c2b9c504",
   "metadata": {},
   "outputs": [],
   "source": [
    "(train, test )= data.randomSplit([0.8, 0.2], seed=10)\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b7c04552-69cf-49cf-8f41-af3f8a5620b8",
   "metadata": {
    "tags": []
   },
   "source": [
    "[train.count(), test.count()]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8e32e08-3627-46a5-8b48-9ce1423a6858",
   "metadata": {},
   "source": [
    "Logistic Regression :\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6b108b64-f600-48d3-945c-6edb25fc0274",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/05/19 18:08:05 WARN DAGScheduler: Broadcasting large task binary with size 21.9 MiB\n",
      "23/05/19 18:08:08 ERROR Executor: Exception in task 3.0 in stage 70.0 (TID 113)]\n",
      "org.apache.spark.SparkException: [FAILED_EXECUTE_UDF] Failed to execute user defined function (VectorAssembler$$Lambda$4062/95814728: (struct<code_client:double,TEL_MOBILE:double,code_pret_F:double,date_de_deboursement_F:double,produit_F:double,agence_F:double,montant_demander:double,montant_accorder:double,duree_F:double,valider_par_F:double,secteur_activite_F:double,code_secteur_du_client:double,code_secteur_projet:double,gouvernorat_F:double,status_pret_F:double,Accor_Payoff_F:double,id:double,ACCOUNT_OFFICER:double,INDUSTRY:double,NATIONALITY_F:double,GENDER_F:double,SECTOR:double,MARITAL_STATUS_F:double,LR_CARTE_BQ_F:double,LR_CUS_MNT_REV:double,SUBURB_TOWN:double,AGE_double_VectorAssembler_874f1c3c72ec:double>) => struct<type:tinyint,size:int,indices:array<int>,values:array<double>>).\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala:217)\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.sort_addToSorter_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.TraversableOnce.foldLeft(TraversableOnce.scala:199)\n",
      "\tat scala.collection.TraversableOnce.foldLeft$(TraversableOnce.scala:192)\n",
      "\tat scala.collection.AbstractIterator.foldLeft(Iterator.scala:1431)\n",
      "\tat scala.collection.TraversableOnce.aggregate(TraversableOnce.scala:260)\n",
      "\tat scala.collection.TraversableOnce.aggregate$(TraversableOnce.scala:260)\n",
      "\tat scala.collection.AbstractIterator.aggregate(Iterator.scala:1431)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$treeAggregate$4(RDD.scala:1234)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$treeAggregate$6(RDD.scala:1235)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:853)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:853)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: org.apache.spark.SparkException: Encountered null while assembling a row with handleInvalid = \"error\". Consider\n",
      "removing nulls from dataset or using handleInvalid = \"keep\" or \"skip\".\n",
      "\tat org.apache.spark.ml.feature.VectorAssembler$.$anonfun$assemble$1(VectorAssembler.scala:291)\n",
      "\tat org.apache.spark.ml.feature.VectorAssembler$.$anonfun$assemble$1$adapted(VectorAssembler.scala:260)\n",
      "\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\n",
      "\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\n",
      "\tat scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:38)\n",
      "\tat org.apache.spark.ml.feature.VectorAssembler$.assemble(VectorAssembler.scala:260)\n",
      "\tat org.apache.spark.ml.feature.VectorAssembler.$anonfun$transform$6(VectorAssembler.scala:143)\n",
      "\t... 32 more\n",
      "23/05/19 18:08:08 WARN TaskSetManager: Lost task 3.0 in stage 70.0 (TID 113) (b0e1bb9c76b4 executor driver): org.apache.spark.SparkException: [FAILED_EXECUTE_UDF] Failed to execute user defined function (VectorAssembler$$Lambda$4062/95814728: (struct<code_client:double,TEL_MOBILE:double,code_pret_F:double,date_de_deboursement_F:double,produit_F:double,agence_F:double,montant_demander:double,montant_accorder:double,duree_F:double,valider_par_F:double,secteur_activite_F:double,code_secteur_du_client:double,code_secteur_projet:double,gouvernorat_F:double,status_pret_F:double,Accor_Payoff_F:double,id:double,ACCOUNT_OFFICER:double,INDUSTRY:double,NATIONALITY_F:double,GENDER_F:double,SECTOR:double,MARITAL_STATUS_F:double,LR_CARTE_BQ_F:double,LR_CUS_MNT_REV:double,SUBURB_TOWN:double,AGE_double_VectorAssembler_874f1c3c72ec:double>) => struct<type:tinyint,size:int,indices:array<int>,values:array<double>>).\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala:217)\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.sort_addToSorter_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.TraversableOnce.foldLeft(TraversableOnce.scala:199)\n",
      "\tat scala.collection.TraversableOnce.foldLeft$(TraversableOnce.scala:192)\n",
      "\tat scala.collection.AbstractIterator.foldLeft(Iterator.scala:1431)\n",
      "\tat scala.collection.TraversableOnce.aggregate(TraversableOnce.scala:260)\n",
      "\tat scala.collection.TraversableOnce.aggregate$(TraversableOnce.scala:260)\n",
      "\tat scala.collection.AbstractIterator.aggregate(Iterator.scala:1431)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$treeAggregate$4(RDD.scala:1234)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$treeAggregate$6(RDD.scala:1235)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:853)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:853)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: org.apache.spark.SparkException: Encountered null while assembling a row with handleInvalid = \"error\". Consider\n",
      "removing nulls from dataset or using handleInvalid = \"keep\" or \"skip\".\n",
      "\tat org.apache.spark.ml.feature.VectorAssembler$.$anonfun$assemble$1(VectorAssembler.scala:291)\n",
      "\tat org.apache.spark.ml.feature.VectorAssembler$.$anonfun$assemble$1$adapted(VectorAssembler.scala:260)\n",
      "\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\n",
      "\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\n",
      "\tat scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:38)\n",
      "\tat org.apache.spark.ml.feature.VectorAssembler$.assemble(VectorAssembler.scala:260)\n",
      "\tat org.apache.spark.ml.feature.VectorAssembler.$anonfun$transform$6(VectorAssembler.scala:143)\n",
      "\t... 32 more\n",
      "\n",
      "23/05/19 18:08:08 ERROR TaskSetManager: Task 3 in stage 70.0 failed 1 times; aborting job\n",
      "23/05/19 18:08:08 ERROR Instrumentation: org.apache.spark.SparkException: Job aborted due to stage failure: Task 3 in stage 70.0 failed 1 times, most recent failure: Lost task 3.0 in stage 70.0 (TID 113) (b0e1bb9c76b4 executor driver): org.apache.spark.SparkException: [FAILED_EXECUTE_UDF] Failed to execute user defined function (VectorAssembler$Lambda$4062/95814728: (struct<code_client:double,TEL_MOBILE:double,code_pret_F:double,date_de_deboursement_F:double,produit_F:double,agence_F:double,montant_demander:double,montant_accorder:double,duree_F:double,valider_par_F:double,secteur_activite_F:double,code_secteur_du_client:double,code_secteur_projet:double,gouvernorat_F:double,status_pret_F:double,Accor_Payoff_F:double,id:double,ACCOUNT_OFFICER:double,INDUSTRY:double,NATIONALITY_F:double,GENDER_F:double,SECTOR:double,MARITAL_STATUS_F:double,LR_CARTE_BQ_F:double,LR_CUS_MNT_REV:double,SUBURB_TOWN:double,AGE_double_VectorAssembler_874f1c3c72ec:double>) => struct<type:tinyint,size:int,indices:array<int>,values:array<double>>).\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala:217)\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.sort_addToSorter_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.TraversableOnce.foldLeft(TraversableOnce.scala:199)\n",
      "\tat scala.collection.TraversableOnce.foldLeft$(TraversableOnce.scala:192)\n",
      "\tat scala.collection.AbstractIterator.foldLeft(Iterator.scala:1431)\n",
      "\tat scala.collection.TraversableOnce.aggregate(TraversableOnce.scala:260)\n",
      "\tat scala.collection.TraversableOnce.aggregate$(TraversableOnce.scala:260)\n",
      "\tat scala.collection.AbstractIterator.aggregate(Iterator.scala:1431)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$treeAggregate$4(RDD.scala:1234)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$treeAggregate$6(RDD.scala:1235)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:853)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:853)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: org.apache.spark.SparkException: Encountered null while assembling a row with handleInvalid = \"error\". Consider\n",
      "removing nulls from dataset or using handleInvalid = \"keep\" or \"skip\".\n",
      "\tat org.apache.spark.ml.feature.VectorAssembler$.$anonfun$assemble$1(VectorAssembler.scala:291)\n",
      "\tat org.apache.spark.ml.feature.VectorAssembler$.$anonfun$assemble$1$adapted(VectorAssembler.scala:260)\n",
      "\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\n",
      "\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\n",
      "\tat scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:38)\n",
      "\tat org.apache.spark.ml.feature.VectorAssembler$.assemble(VectorAssembler.scala:260)\n",
      "\tat org.apache.spark.ml.feature.VectorAssembler.$anonfun$transform$6(VectorAssembler.scala:143)\n",
      "\t... 32 more\n",
      "\n",
      "Driver stacktrace:\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2785)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2721)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2720)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2720)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1206)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1206)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1206)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2984)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2923)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2912)\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:971)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2263)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2358)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$fold$1(RDD.scala:1172)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
      "\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:405)\n",
      "\tat org.apache.spark.rdd.RDD.fold(RDD.scala:1166)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$treeAggregate$2(RDD.scala:1259)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
      "\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:405)\n",
      "\tat org.apache.spark.rdd.RDD.treeAggregate(RDD.scala:1226)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$treeAggregate$1(RDD.scala:1212)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
      "\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:405)\n",
      "\tat org.apache.spark.rdd.RDD.treeAggregate(RDD.scala:1212)\n",
      "\tat org.apache.spark.ml.stat.Summarizer$.getClassificationSummarizers(Summarizer.scala:233)\n",
      "\tat org.apache.spark.ml.classification.LogisticRegression.$anonfun$train$1(LogisticRegression.scala:517)\n",
      "\tat org.apache.spark.ml.util.Instrumentation$.$anonfun$instrumented$1(Instrumentation.scala:191)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:191)\n",
      "\tat org.apache.spark.ml.classification.LogisticRegression.train(LogisticRegression.scala:497)\n",
      "\tat org.apache.spark.ml.classification.LogisticRegression.train(LogisticRegression.scala:287)\n",
      "\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:114)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: org.apache.spark.SparkException: [FAILED_EXECUTE_UDF] Failed to execute user defined function (VectorAssembler$$Lambda$4062/95814728: (struct<code_client:double,TEL_MOBILE:double,code_pret_F:double,date_de_deboursement_F:double,produit_F:double,agence_F:double,montant_demander:double,montant_accorder:double,duree_F:double,valider_par_F:double,secteur_activite_F:double,code_secteur_du_client:double,code_secteur_projet:double,gouvernorat_F:double,status_pret_F:double,Accor_Payoff_F:double,id:double,ACCOUNT_OFFICER:double,INDUSTRY:double,NATIONALITY_F:double,GENDER_F:double,SECTOR:double,MARITAL_STATUS_F:double,LR_CARTE_BQ_F:double,LR_CUS_MNT_REV:double,SUBURB_TOWN:double,AGE_double_VectorAssembler_874f1c3c72ec:double>) => struct<type:tinyint,size:int,indices:array<int>,values:array<double>>).\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala:217)\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.sort_addToSorter_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.TraversableOnce.foldLeft(TraversableOnce.scala:199)\n",
      "\tat scala.collection.TraversableOnce.foldLeft$(TraversableOnce.scala:192)\n",
      "\tat scala.collection.AbstractIterator.foldLeft(Iterator.scala:1431)\n",
      "\tat scala.collection.TraversableOnce.aggregate(TraversableOnce.scala:260)\n",
      "\tat scala.collection.TraversableOnce.aggregate$(TraversableOnce.scala:260)\n",
      "\tat scala.collection.AbstractIterator.aggregate(Iterator.scala:1431)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$treeAggregate$4(RDD.scala:1234)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$treeAggregate$6(RDD.scala:1235)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:853)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:853)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\t... 1 more\n",
      "Caused by: org.apache.spark.SparkException: Encountered null while assembling a row with handleInvalid = \"error\". Consider\n",
      "removing nulls from dataset or using handleInvalid = \"keep\" or \"skip\".\n",
      "\tat org.apache.spark.ml.feature.VectorAssembler$.$anonfun$assemble$1(VectorAssembler.scala:291)\n",
      "\tat org.apache.spark.ml.feature.VectorAssembler$.$anonfun$assemble$1$adapted(VectorAssembler.scala:260)\n",
      "\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\n",
      "\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\n",
      "\tat scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:38)\n",
      "\tat org.apache.spark.ml.feature.VectorAssembler$.assemble(VectorAssembler.scala:260)\n",
      "\tat org.apache.spark.ml.feature.VectorAssembler.$anonfun$transform$6(VectorAssembler.scala:143)\n",
      "\t... 32 more\n",
      "\n",
      "23/05/19 18:08:08 WARN TaskSetManager: Lost task 0.0 in stage 70.0 (TID 110) (b0e1bb9c76b4 executor driver): TaskKilled (Stage cancelled)\n",
      "23/05/19 18:08:08 WARN TaskSetManager: Lost task 1.0 in stage 70.0 (TID 111) (b0e1bb9c76b4 executor driver): TaskKilled (Stage cancelled)\n",
      "23/05/19 18:08:08 WARN TaskSetManager: Lost task 2.0 in stage 70.0 (TID 112) (b0e1bb9c76b4 executor driver): TaskKilled (Stage cancelled)\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o952.fit.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 3 in stage 70.0 failed 1 times, most recent failure: Lost task 3.0 in stage 70.0 (TID 113) (b0e1bb9c76b4 executor driver): org.apache.spark.SparkException: [FAILED_EXECUTE_UDF] Failed to execute user defined function (VectorAssembler$$Lambda$4062/95814728: (struct<code_client:double,TEL_MOBILE:double,code_pret_F:double,date_de_deboursement_F:double,produit_F:double,agence_F:double,montant_demander:double,montant_accorder:double,duree_F:double,valider_par_F:double,secteur_activite_F:double,code_secteur_du_client:double,code_secteur_projet:double,gouvernorat_F:double,status_pret_F:double,Accor_Payoff_F:double,id:double,ACCOUNT_OFFICER:double,INDUSTRY:double,NATIONALITY_F:double,GENDER_F:double,SECTOR:double,MARITAL_STATUS_F:double,LR_CARTE_BQ_F:double,LR_CUS_MNT_REV:double,SUBURB_TOWN:double,AGE_double_VectorAssembler_874f1c3c72ec:double>) => struct<type:tinyint,size:int,indices:array<int>,values:array<double>>).\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala:217)\n\tat org.apache.spark.sql.errors.QueryExecutionErrors.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.sort_addToSorter_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n\tat scala.collection.TraversableOnce.foldLeft(TraversableOnce.scala:199)\n\tat scala.collection.TraversableOnce.foldLeft$(TraversableOnce.scala:192)\n\tat scala.collection.AbstractIterator.foldLeft(Iterator.scala:1431)\n\tat scala.collection.TraversableOnce.aggregate(TraversableOnce.scala:260)\n\tat scala.collection.TraversableOnce.aggregate$(TraversableOnce.scala:260)\n\tat scala.collection.AbstractIterator.aggregate(Iterator.scala:1431)\n\tat org.apache.spark.rdd.RDD.$anonfun$treeAggregate$4(RDD.scala:1234)\n\tat org.apache.spark.rdd.RDD.$anonfun$treeAggregate$6(RDD.scala:1235)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:853)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:853)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: org.apache.spark.SparkException: Encountered null while assembling a row with handleInvalid = \"error\". Consider\nremoving nulls from dataset or using handleInvalid = \"keep\" or \"skip\".\n\tat org.apache.spark.ml.feature.VectorAssembler$.$anonfun$assemble$1(VectorAssembler.scala:291)\n\tat org.apache.spark.ml.feature.VectorAssembler$.$anonfun$assemble$1$adapted(VectorAssembler.scala:260)\n\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\n\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\n\tat scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:38)\n\tat org.apache.spark.ml.feature.VectorAssembler$.assemble(VectorAssembler.scala:260)\n\tat org.apache.spark.ml.feature.VectorAssembler.$anonfun$transform$6(VectorAssembler.scala:143)\n\t... 32 more\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2785)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2721)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2720)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2720)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1206)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1206)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1206)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2984)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2923)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2912)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:971)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2263)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2358)\n\tat org.apache.spark.rdd.RDD.$anonfun$fold$1(RDD.scala:1172)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:405)\n\tat org.apache.spark.rdd.RDD.fold(RDD.scala:1166)\n\tat org.apache.spark.rdd.RDD.$anonfun$treeAggregate$2(RDD.scala:1259)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:405)\n\tat org.apache.spark.rdd.RDD.treeAggregate(RDD.scala:1226)\n\tat org.apache.spark.rdd.RDD.$anonfun$treeAggregate$1(RDD.scala:1212)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:405)\n\tat org.apache.spark.rdd.RDD.treeAggregate(RDD.scala:1212)\n\tat org.apache.spark.ml.stat.Summarizer$.getClassificationSummarizers(Summarizer.scala:233)\n\tat org.apache.spark.ml.classification.LogisticRegression.$anonfun$train$1(LogisticRegression.scala:517)\n\tat org.apache.spark.ml.util.Instrumentation$.$anonfun$instrumented$1(Instrumentation.scala:191)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:191)\n\tat org.apache.spark.ml.classification.LogisticRegression.train(LogisticRegression.scala:497)\n\tat org.apache.spark.ml.classification.LogisticRegression.train(LogisticRegression.scala:287)\n\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:114)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: org.apache.spark.SparkException: [FAILED_EXECUTE_UDF] Failed to execute user defined function (VectorAssembler$$Lambda$4062/95814728: (struct<code_client:double,TEL_MOBILE:double,code_pret_F:double,date_de_deboursement_F:double,produit_F:double,agence_F:double,montant_demander:double,montant_accorder:double,duree_F:double,valider_par_F:double,secteur_activite_F:double,code_secteur_du_client:double,code_secteur_projet:double,gouvernorat_F:double,status_pret_F:double,Accor_Payoff_F:double,id:double,ACCOUNT_OFFICER:double,INDUSTRY:double,NATIONALITY_F:double,GENDER_F:double,SECTOR:double,MARITAL_STATUS_F:double,LR_CARTE_BQ_F:double,LR_CUS_MNT_REV:double,SUBURB_TOWN:double,AGE_double_VectorAssembler_874f1c3c72ec:double>) => struct<type:tinyint,size:int,indices:array<int>,values:array<double>>).\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala:217)\n\tat org.apache.spark.sql.errors.QueryExecutionErrors.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.sort_addToSorter_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n\tat scala.collection.TraversableOnce.foldLeft(TraversableOnce.scala:199)\n\tat scala.collection.TraversableOnce.foldLeft$(TraversableOnce.scala:192)\n\tat scala.collection.AbstractIterator.foldLeft(Iterator.scala:1431)\n\tat scala.collection.TraversableOnce.aggregate(TraversableOnce.scala:260)\n\tat scala.collection.TraversableOnce.aggregate$(TraversableOnce.scala:260)\n\tat scala.collection.AbstractIterator.aggregate(Iterator.scala:1431)\n\tat org.apache.spark.rdd.RDD.$anonfun$treeAggregate$4(RDD.scala:1234)\n\tat org.apache.spark.rdd.RDD.$anonfun$treeAggregate$6(RDD.scala:1235)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:853)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:853)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\nCaused by: org.apache.spark.SparkException: Encountered null while assembling a row with handleInvalid = \"error\". Consider\nremoving nulls from dataset or using handleInvalid = \"keep\" or \"skip\".\n\tat org.apache.spark.ml.feature.VectorAssembler$.$anonfun$assemble$1(VectorAssembler.scala:291)\n\tat org.apache.spark.ml.feature.VectorAssembler$.$anonfun$assemble$1$adapted(VectorAssembler.scala:260)\n\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\n\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\n\tat scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:38)\n\tat org.apache.spark.ml.feature.VectorAssembler$.assemble(VectorAssembler.scala:260)\n\tat org.apache.spark.ml.feature.VectorAssembler.$anonfun$transform$6(VectorAssembler.scala:143)\n\t... 32 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[35], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m logistic \u001b[38;5;241m=\u001b[39m LogisticRegression(labelCol \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFraude\u001b[39m\u001b[38;5;124m\"\u001b[39m, featuresCol \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfeatures\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Learn from the training data.\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m lrModel \u001b[38;5;241m=\u001b[39m \u001b[43mlogistic\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/pyspark/ml/base.py:205\u001b[0m, in \u001b[0;36mEstimator.fit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    203\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy(params)\u001b[38;5;241m.\u001b[39m_fit(dataset)\n\u001b[1;32m    204\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 205\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    207\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m    208\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mParams must be either a param map or a list/tuple of param maps, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    209\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut got \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mtype\u001b[39m(params)\n\u001b[1;32m    210\u001b[0m     )\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/pyspark/ml/wrapper.py:381\u001b[0m, in \u001b[0;36mJavaEstimator._fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    380\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_fit\u001b[39m(\u001b[38;5;28mself\u001b[39m, dataset: DataFrame) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m JM:\n\u001b[0;32m--> 381\u001b[0m     java_model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_java\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    382\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_model(java_model)\n\u001b[1;32m    383\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_copyValues(model)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/pyspark/ml/wrapper.py:378\u001b[0m, in \u001b[0;36mJavaEstimator._fit_java\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    375\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_java_obj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    377\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transfer_params_to_java()\n\u001b[0;32m--> 378\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_java_obj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/pyspark/errors/exceptions/captured.py:169\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    167\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    168\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 169\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    170\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    171\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o952.fit.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 3 in stage 70.0 failed 1 times, most recent failure: Lost task 3.0 in stage 70.0 (TID 113) (b0e1bb9c76b4 executor driver): org.apache.spark.SparkException: [FAILED_EXECUTE_UDF] Failed to execute user defined function (VectorAssembler$$Lambda$4062/95814728: (struct<code_client:double,TEL_MOBILE:double,code_pret_F:double,date_de_deboursement_F:double,produit_F:double,agence_F:double,montant_demander:double,montant_accorder:double,duree_F:double,valider_par_F:double,secteur_activite_F:double,code_secteur_du_client:double,code_secteur_projet:double,gouvernorat_F:double,status_pret_F:double,Accor_Payoff_F:double,id:double,ACCOUNT_OFFICER:double,INDUSTRY:double,NATIONALITY_F:double,GENDER_F:double,SECTOR:double,MARITAL_STATUS_F:double,LR_CARTE_BQ_F:double,LR_CUS_MNT_REV:double,SUBURB_TOWN:double,AGE_double_VectorAssembler_874f1c3c72ec:double>) => struct<type:tinyint,size:int,indices:array<int>,values:array<double>>).\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala:217)\n\tat org.apache.spark.sql.errors.QueryExecutionErrors.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.sort_addToSorter_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n\tat scala.collection.TraversableOnce.foldLeft(TraversableOnce.scala:199)\n\tat scala.collection.TraversableOnce.foldLeft$(TraversableOnce.scala:192)\n\tat scala.collection.AbstractIterator.foldLeft(Iterator.scala:1431)\n\tat scala.collection.TraversableOnce.aggregate(TraversableOnce.scala:260)\n\tat scala.collection.TraversableOnce.aggregate$(TraversableOnce.scala:260)\n\tat scala.collection.AbstractIterator.aggregate(Iterator.scala:1431)\n\tat org.apache.spark.rdd.RDD.$anonfun$treeAggregate$4(RDD.scala:1234)\n\tat org.apache.spark.rdd.RDD.$anonfun$treeAggregate$6(RDD.scala:1235)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:853)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:853)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: org.apache.spark.SparkException: Encountered null while assembling a row with handleInvalid = \"error\". Consider\nremoving nulls from dataset or using handleInvalid = \"keep\" or \"skip\".\n\tat org.apache.spark.ml.feature.VectorAssembler$.$anonfun$assemble$1(VectorAssembler.scala:291)\n\tat org.apache.spark.ml.feature.VectorAssembler$.$anonfun$assemble$1$adapted(VectorAssembler.scala:260)\n\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\n\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\n\tat scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:38)\n\tat org.apache.spark.ml.feature.VectorAssembler$.assemble(VectorAssembler.scala:260)\n\tat org.apache.spark.ml.feature.VectorAssembler.$anonfun$transform$6(VectorAssembler.scala:143)\n\t... 32 more\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2785)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2721)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2720)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2720)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1206)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1206)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1206)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2984)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2923)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2912)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:971)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2263)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2358)\n\tat org.apache.spark.rdd.RDD.$anonfun$fold$1(RDD.scala:1172)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:405)\n\tat org.apache.spark.rdd.RDD.fold(RDD.scala:1166)\n\tat org.apache.spark.rdd.RDD.$anonfun$treeAggregate$2(RDD.scala:1259)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:405)\n\tat org.apache.spark.rdd.RDD.treeAggregate(RDD.scala:1226)\n\tat org.apache.spark.rdd.RDD.$anonfun$treeAggregate$1(RDD.scala:1212)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:405)\n\tat org.apache.spark.rdd.RDD.treeAggregate(RDD.scala:1212)\n\tat org.apache.spark.ml.stat.Summarizer$.getClassificationSummarizers(Summarizer.scala:233)\n\tat org.apache.spark.ml.classification.LogisticRegression.$anonfun$train$1(LogisticRegression.scala:517)\n\tat org.apache.spark.ml.util.Instrumentation$.$anonfun$instrumented$1(Instrumentation.scala:191)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:191)\n\tat org.apache.spark.ml.classification.LogisticRegression.train(LogisticRegression.scala:497)\n\tat org.apache.spark.ml.classification.LogisticRegression.train(LogisticRegression.scala:287)\n\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:114)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: org.apache.spark.SparkException: [FAILED_EXECUTE_UDF] Failed to execute user defined function (VectorAssembler$$Lambda$4062/95814728: (struct<code_client:double,TEL_MOBILE:double,code_pret_F:double,date_de_deboursement_F:double,produit_F:double,agence_F:double,montant_demander:double,montant_accorder:double,duree_F:double,valider_par_F:double,secteur_activite_F:double,code_secteur_du_client:double,code_secteur_projet:double,gouvernorat_F:double,status_pret_F:double,Accor_Payoff_F:double,id:double,ACCOUNT_OFFICER:double,INDUSTRY:double,NATIONALITY_F:double,GENDER_F:double,SECTOR:double,MARITAL_STATUS_F:double,LR_CARTE_BQ_F:double,LR_CUS_MNT_REV:double,SUBURB_TOWN:double,AGE_double_VectorAssembler_874f1c3c72ec:double>) => struct<type:tinyint,size:int,indices:array<int>,values:array<double>>).\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala:217)\n\tat org.apache.spark.sql.errors.QueryExecutionErrors.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.sort_addToSorter_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n\tat scala.collection.TraversableOnce.foldLeft(TraversableOnce.scala:199)\n\tat scala.collection.TraversableOnce.foldLeft$(TraversableOnce.scala:192)\n\tat scala.collection.AbstractIterator.foldLeft(Iterator.scala:1431)\n\tat scala.collection.TraversableOnce.aggregate(TraversableOnce.scala:260)\n\tat scala.collection.TraversableOnce.aggregate$(TraversableOnce.scala:260)\n\tat scala.collection.AbstractIterator.aggregate(Iterator.scala:1431)\n\tat org.apache.spark.rdd.RDD.$anonfun$treeAggregate$4(RDD.scala:1234)\n\tat org.apache.spark.rdd.RDD.$anonfun$treeAggregate$6(RDD.scala:1235)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:853)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:853)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\nCaused by: org.apache.spark.SparkException: Encountered null while assembling a row with handleInvalid = \"error\". Consider\nremoving nulls from dataset or using handleInvalid = \"keep\" or \"skip\".\n\tat org.apache.spark.ml.feature.VectorAssembler$.$anonfun$assemble$1(VectorAssembler.scala:291)\n\tat org.apache.spark.ml.feature.VectorAssembler$.$anonfun$assemble$1$adapted(VectorAssembler.scala:260)\n\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\n\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\n\tat scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:38)\n\tat org.apache.spark.ml.feature.VectorAssembler$.assemble(VectorAssembler.scala:260)\n\tat org.apache.spark.ml.feature.VectorAssembler.$anonfun$transform$6(VectorAssembler.scala:143)\n\t... 32 more\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "#Create a Logistic Regression classifier.\n",
    "logistic = LogisticRegression(labelCol = \"Fraude\", featuresCol = \"features\")\n",
    "# Learn from the training data.\n",
    "lrModel = logistic.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58944931-6243-4071-bbf7-da764605006a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prediction_LR = lrModel.transform(test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82ec2aff-b819-472e-aa1e-fc22ca7da8ea",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
