{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bc561016-8402-4061-a687-596a212005a4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/05/19 20:56:45 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://b0e1bb9c76b4:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.4.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>flask</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f17437d0700>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/05/19 21:06:42 WARN JavaUtils: Attempt to delete using native Unix OS command failed for path = /tmp/spark-f103a537-f86b-46a0-81fc-18aecd6c55aa. Falling back to Java IO way\n",
      "java.io.IOException: Failed to delete: /tmp/spark-f103a537-f86b-46a0-81fc-18aecd6c55aa\n",
      "\tat org.apache.spark.network.util.JavaUtils.deleteRecursivelyUsingUnixNative(JavaUtils.java:177)\n",
      "\tat org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:113)\n",
      "\tat org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:94)\n",
      "\tat org.apache.spark.util.Utils$.deleteRecursively(Utils.scala:1231)\n",
      "\tat org.apache.spark.util.ShutdownHookManager$.$anonfun$new$4(ShutdownHookManager.scala:65)\n",
      "\tat org.apache.spark.util.ShutdownHookManager$.$anonfun$new$4$adapted(ShutdownHookManager.scala:62)\n",
      "\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\n",
      "\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\n",
      "\tat scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:198)\n",
      "\tat org.apache.spark.util.ShutdownHookManager$.$anonfun$new$2(ShutdownHookManager.scala:62)\n",
      "\tat org.apache.spark.util.SparkShutdownHook.run(ShutdownHookManager.scala:214)\n",
      "\tat org.apache.spark.util.SparkShutdownHookManager.$anonfun$runAll$2(ShutdownHookManager.scala:188)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2088)\n",
      "\tat org.apache.spark.util.SparkShutdownHookManager.$anonfun$runAll$1(ShutdownHookManager.scala:188)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.util.SparkShutdownHookManager.runAll(ShutdownHookManager.scala:188)\n",
      "\tat org.apache.spark.util.SparkShutdownHookManager$$anon$2.run(ShutdownHookManager.scala:178)\n",
      "\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n",
      "\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import os\n",
    "os.environ['HADOOP_CONF_DIR'] = '/opt/conf'\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "        .appName('flask') \\\n",
    "        .getOrCreate()\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "122e639b-dde7-455e-bf78-fe77d89b6de3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+---------+-----------+---+----------+\n",
      "|  _c0|       _c1|      _c2|        _c3|_c4|       _c5|\n",
      "+-----+----------+---------+-----------+---+----------+\n",
      "|10001|1953-09-02|   Georgi|    Facello|  M|1986-06-26|\n",
      "|10002|1964-06-02|  Bezalel|     Simmel|  F|1985-11-21|\n",
      "|10003|1959-12-03|    Parto|    Bamford|  M|1986-08-28|\n",
      "|10004|1954-05-01|Chirstian|    Koblick|  M|1986-12-01|\n",
      "|10005|1955-01-21|  Kyoichi|   Maliniak|  M|1989-09-12|\n",
      "|10006|1953-04-20|   Anneke|    Preusig|  F|1989-06-02|\n",
      "|10007|1957-05-23|  Tzvetan|  Zielinski|  F|1989-02-10|\n",
      "|10008|1958-02-19|   Saniya|   Kalloufi|  M|1994-09-15|\n",
      "|10009|1952-04-19|   Sumant|       Peac|  F|1985-02-18|\n",
      "|10010|1963-06-01|Duangkaew|   Piveteau|  F|1989-08-24|\n",
      "|10011|1953-11-07|     Mary|      Sluis|  F|1990-01-22|\n",
      "|10012|1960-10-04| Patricio|  Bridgland|  M|1992-12-18|\n",
      "|10013|1963-06-07|Eberhardt|     Terkki|  M|1985-10-20|\n",
      "|10014|1956-02-12|    Berni|      Genin|  M|1987-03-11|\n",
      "|10015|1959-08-19| Guoxiang|  Nooteboom|  M|1987-07-02|\n",
      "|10016|1961-05-02| Kazuhito|Cappelletti|  M|1995-01-27|\n",
      "|10017|1958-07-06|Cristinel|  Bouloucos|  F|1993-08-03|\n",
      "|10018|1954-06-19| Kazuhide|       Peha|  F|1987-04-03|\n",
      "|10019|1953-01-23|  Lillian|    Haddadi|  M|1999-04-30|\n",
      "|10020|1952-12-24|   Mayuko|    Warwick|  M|1991-01-26|\n",
      "+-----+----------+---------+-----------+---+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
